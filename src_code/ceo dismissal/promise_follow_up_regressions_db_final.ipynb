{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7ab26",
   "metadata": {},
   "source": [
    "# Broken Promises Count DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63256acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Load and prepare data (unchanged)\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "promises = pd.read_csv(\"../data/sxp1500_presentations_ceo_aggregated_promises_expanded_cleaned_transcriptlevel_horizon_specificity.csv\")\n",
    "# create promise_id column, it is gvkey_transcriptid_2digitnumber (01, 02, 03, ...)\n",
    "promises['promise_id'] = promises.groupby(['gvkey', 'transcriptid']).cumcount() + 1\n",
    "promises['promise_id'] = promises['gvkey'].astype(str) + '_' + promises['transcriptid'].astype(str) + '_' + promises['promise_id'].apply(lambda x: f'{x:02d}')\n",
    "promises_select = promises[['gvkey', 'mostimportantdateutc', 'companyname', 'exec_fullname', 'execid', 'promise_id','1-promise-verbatim' ,'2-promise-explain' ,'3-promise-horizon-v2', 'specificity_score']].sort_values(by=['gvkey', 'mostimportantdateutc',])\n",
    "\n",
    "labels = pd.read_csv(\"promises_with_keywords_v5_labels.csv\")\n",
    "def revert_promise_id(promise_id):\n",
    "    parts = promise_id.split('_')\n",
    "    fixed_parts = []\n",
    "    for part in parts:\n",
    "        if part.endswith('.0'):\n",
    "            part = str(int(float(part)))\n",
    "        fixed_parts.append(part)\n",
    "    return '_'.join(fixed_parts)\n",
    "\n",
    "labels['promise_id'] = labels['promise_id'].apply(revert_promise_id)\n",
    "\n",
    "labels = labels[['promise_id', 'primary_keyword']]\n",
    "\n",
    "# merge promises and labels on promise_id\n",
    "promises_select = pd.merge(labels, promises_select, on=['promise_id'], how='left')\n",
    "\n",
    "# %%\n",
    "promises_select.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "promises_select_10percent_results = pd.read_csv(\"promises_select_10percent_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e78af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with promises_select_10percent on promise_id\n",
    "promises_select_10percent_results_merged = pd.merge(promises_select_10percent_results, promises_select, on=['promise_id'], how='left')\n",
    "\n",
    "# %%\n",
    "\n",
    "promises_select_10percent_results_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0756c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "promises_select_10percent_results_merged.status_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44df07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = promises_select_10percent_results_merged.copy()\n",
    "\n",
    "df['date'] = pd.to_datetime(df['mostimportantdateutc'], errors='coerce')\n",
    "df['year'] = df['date'].dt.year\n",
    "df['is_broken'] = df['status_code'].isin(['DELAYED', 'NOT_DELIVERED']).astype(int)\n",
    "\n",
    "# per exec-year: broken count and total promises\n",
    "by_year = (df.groupby(['execid', 'year'])\n",
    "             .agg(broken_count=('is_broken', 'sum'),\n",
    "                  promises_in_year=('execid', 'size'))\n",
    "             .reset_index())\n",
    "\n",
    "# cumulative *prior* years\n",
    "by_year = by_year.sort_values(['execid', 'year'])\n",
    "by_year['no_broken_promises'] = (\n",
    "    by_year.groupby('execid')['broken_count'].cumsum().shift(1, fill_value=0).astype(int)\n",
    ")\n",
    "by_year['no_promises_prior'] = (\n",
    "    by_year.groupby('execid')['promises_in_year'].cumsum().shift(1, fill_value=0).astype(int)\n",
    ")\n",
    "\n",
    "# --- Rolling windows over prior calendar years (2, 3, 5) ---\n",
    "# We compute on a full year grid per execid so missing years contribute 0,\n",
    "# then merge back to the original rows (execid, year).\n",
    "roll_list = []\n",
    "for eid, g in by_year.groupby('execid', sort=False):\n",
    "    # full calendar grid for this exec\n",
    "    yr_min, yr_max = int(g['year'].min()), int(g['year'].max())\n",
    "    idx = pd.RangeIndex(yr_min, yr_max + 1, 1, name='year')\n",
    "\n",
    "    tmp = (g.set_index('year')[['broken_count', 'promises_in_year']]\n",
    "             .reindex(idx, fill_value=0))\n",
    "\n",
    "    # exclude current year using shift(1); window counts previous N years only\n",
    "    for w in (2, 3, 5):\n",
    "        tmp[f'no_broken_promises_roll{w}'] = (\n",
    "            tmp['broken_count'].rolling(window=w, min_periods=1).sum()\n",
    "            .shift(1, fill_value=0)\n",
    "        ).astype(int)\n",
    "        tmp[f'no_promises_prior_roll{w}'] = (\n",
    "            tmp['promises_in_year'].rolling(window=w, min_periods=1).sum()\n",
    "            .shift(1, fill_value=0)\n",
    "        ).astype(int)\n",
    "\n",
    "    tmp['execid'] = eid\n",
    "    roll_list.append(tmp.reset_index()[['execid', 'year',\n",
    "        'no_broken_promises_roll2', 'no_broken_promises_roll3', 'no_broken_promises_roll5',\n",
    "        'no_promises_prior_roll2', 'no_promises_prior_roll3', 'no_promises_prior_roll5']])\n",
    "\n",
    "roll_df = pd.concat(roll_list, ignore_index=True)\n",
    "\n",
    "# merge rolling features back to the compact per-year table\n",
    "result = (by_year.merge(roll_df, on=['execid', 'year'], how='left')\n",
    "                 [['execid', 'year',\n",
    "                   'no_broken_promises', 'no_promises_prior', 'promises_in_year',\n",
    "                   'no_broken_promises_roll2', 'no_broken_promises_roll3', 'no_broken_promises_roll5',\n",
    "                   'no_promises_prior_roll2', 'no_promises_prior_roll3', 'no_promises_prior_roll5']])\n",
    "\n",
    "# result is your final table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4178fa",
   "metadata": {},
   "source": [
    "# Excecucomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc58221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEO data\n",
    "\n",
    "previously_downloaded_execucomp = True\n",
    "if previously_downloaded_execucomp == False:\n",
    "        \n",
    "        wrds_username = os.getenv('WRDS_USERNAME')\n",
    "        wrds_password = os.getenv('WRDS_PASSWORD')\n",
    "        db_wrds = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    \n",
    "        # replace with wrdssec.forms\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM  comp_execucomp.anncomp\n",
    "            WHERE ceoann='CEO'\n",
    "        \"\"\"\n",
    "        \n",
    "        execucomp = db_wrds.raw_sql(query)\n",
    "        execucomp['gvkey'] = execucomp['gvkey'].astype(int)\n",
    "\n",
    "        execucomp.to_csv('../data/execucomp.csv')\n",
    "        db_wrds.close()\n",
    "\n",
    "else:\n",
    "    execucomp = pd.read_csv('../data/execucomp.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether the title collumn contains chmn or chairman\n",
    "execucomp['ceo_dual'] = execucomp['title'].str.contains('chmn|chairman', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_director'] = execucomp['execdir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_options_compensation'] = execucomp['opt_unex_exer_est_val']/execucomp['salary']\n",
    "execucomp['ceo_options_compensation_log'] = np.log(execucomp['ceo_options_compensation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_option_awards'] = execucomp['option_awards_blk_value']/execucomp['salary']\n",
    "execucomp['ceo_option_awards_log'] = np.log(execucomp['ceo_option_awards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d90122",
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_gender'] = execucomp['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender dummy\n",
    "execucomp['ceo_gender_dummy'] = execucomp['ceo_gender'].apply(lambda x: 1 if x=='MALE' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_age'] = execucomp['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure your DataFrame is sorted\n",
    "execucomp = execucomp.sort_values(by=['gvkey', 'execid', 'year'])\n",
    "\n",
    "# Create a group identifier for consecutive years\n",
    "execucomp['year_group'] = execucomp.groupby(['gvkey', 'execid'])['year'].diff().ne(1).cumsum()\n",
    "\n",
    "# Create the ceo_tenure variable\n",
    "execucomp['ceo_tenure'] = execucomp.groupby(['gvkey', 'execid', 'year_group']).cumcount() + 1\n",
    "\n",
    "# Drop the temporary 'year_group' column\n",
    "execucomp.drop(columns=['year_group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_total_compensation'] = execucomp['salary'] + execucomp['bonus'] + execucomp['othcomp']\n",
    "execucomp['ceo_total_compensation_log'] = np.log(execucomp['ceo_total_compensation'])\n",
    "\n",
    "execucomp['ceo_total_deferred_compensation'] = execucomp['defer_balance_tot'] + execucomp['defer_contrib_co_tot'] + execucomp['defer_contrib_exec_tot']\n",
    "execucomp['ceo_total_deferred_compensation_log'] = np.log(execucomp['ceo_total_deferred_compensation'])\n",
    "\n",
    "execucomp['ceo_total_shares_owned'] = execucomp['shrown_tot']\n",
    "execucomp['ceo_total_shares_owned_log'] = np.log(execucomp['ceo_total_shares_owned'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c709cd",
   "metadata": {},
   "source": [
    "# Compustat Quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_downloaded_compustat_quarterly = True\n",
    "if previously_downloaded_compustat_quarterly == False:\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    # download finratiofirm table\n",
    "    query = \"\"\"\n",
    "    SELECT gvkey, datadate, indfmt, consol, popsrc, datafmt, fyr, actq, atq, ibq, niq, niy, epsfi12, oeps12, epsfxy, mkvaltq, prccq, prchq, prclq, saleq, cshoq, actq, lctq, xoprq, xrdq, intanq, txdbq, atq, dpq, aqpq, dlttq, dlcq, seqq\n",
    "    FROM comp_na_daily_all.fundq\n",
    "    WHERE fyearq >= 2003\n",
    "    \"\"\"\n",
    "    compustat = db.raw_sql(query)\n",
    "    compustat.to_pickle('../data/compustat_q.pkl')\n",
    "\n",
    "else:\n",
    "    # Load the ratio data from the pickle file\n",
    "    compustat = pd.read_pickle('../data/compustat_q.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compustat=compustat[compustat.datafmt=='STD']\n",
    "compustat=compustat[compustat.popsrc=='D']\n",
    "compustat=compustat[compustat.consol=='C']\n",
    "compustat=compustat[compustat.indfmt== 'INDL']\n",
    "\n",
    "compustat['datadate'] = pd.to_datetime(compustat['datadate'], format='%Y%m%d')\n",
    "compustat['month']=compustat['datadate'].dt.month\n",
    "compustat['year']=compustat['datadate'].dt.year\n",
    "compustat.drop_duplicates(['gvkey','datadate'], inplace=True)\n",
    "compustat['gvkey'] = compustat['gvkey'].astype(int)\n",
    "compustat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332616f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPA\n",
    "compustat['EPS'] = np.where(compustat['cshoq'] != 0, compustat['niq'] / compustat['cshoq'], np.nan)\n",
    "\n",
    "# roa\n",
    "compustat['roa'] = np.where(compustat['atq'] != 0, compustat['niq'] / compustat['atq'], np.nan)\n",
    "\n",
    "# roe\n",
    "compustat['roe'] = np.where((compustat['prccq']* compustat['cshoq']) != 0, compustat['niq'] / (compustat['prccq']* compustat['cshoq']), np.nan)\n",
    "\n",
    "\n",
    "# r&d share of total expense\n",
    "compustat['rd_f'] = compustat['xrdq'] / compustat['xoprq'].where(compustat['xoprq'] != 0, np.nan)\n",
    "compustat['rd_f'] = compustat['rd_f'].fillna(0)\n",
    "\n",
    "# recognized intangible assets as part of total assets\n",
    "compustat['intang_f'] = compustat['intanq'] / compustat['atq'].where(compustat['atq'] != 0, np.nan)\n",
    "compustat['intang_f'] = compustat['intang_f'].fillna(0)\n",
    "\n",
    "# depreciation as part of total assets\n",
    "compustat['dpt_f'] = compustat['dpq'] / compustat['atq'].where(compustat['atq'] != 0, np.nan)\n",
    "compustat['dpt_f'] = compustat['dpt_f'].fillna(0)\n",
    "\n",
    "#Total merger\n",
    "compustat['mergers'] = compustat['aqpq'].fillna(0)\n",
    "\n",
    "# Leverage\n",
    "compustat['leverage'] = (compustat['dlttq'] + compustat['dlcq']) / compustat['seqq']\n",
    "compustat.loc[compustat['seqq'] == 0, 'leverage'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# 2.1: Cash flow (CF) = ibq + dpq\n",
    "compustat['cf'] = compustat['ibq'] + compustat['dpq']\n",
    "\n",
    "\n",
    "# 2.2: Market Value of Equity (MVE) = cshoq * prccq\n",
    "compustat['mve'] = compustat['cshoq'] * compustat['prccq']\n",
    "\n",
    "# 2.3: Book Debt \n",
    "#      A common approximation is atq - seqq - txdbq.\n",
    "compustat['book_debt'] = compustat['atq'] - compustat['seqq']\n",
    "compustat['book_debt'] = compustat['book_debt'] - compustat['txdbq'].fillna(0)\n",
    "\n",
    "# 2.4: Tobin's Q = (MVE + Book Debt) / atq\n",
    "compustat['tobin_q'] = (compustat['mve'] + compustat['book_debt']) / compustat['atq']\n",
    "# earnings_volatility\n",
    "compustat.sort_values(['gvkey', 'datadate'], inplace=True)\n",
    "compustat['earnings_volatility'] = compustat.groupby(['gvkey'])['roa'].transform(lambda x: x.rolling(6).std())\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsorize the variables at the 1% level\n",
    "compustat['EPS'] = winsorize(compustat['EPS'], limits=[0.025, 0.025])\n",
    "compustat['roa'] = winsorize(compustat['roa'], limits=[0.025, 0.025])\n",
    "compustat['roe'] = winsorize(compustat['roe'], limits=[0.025, 0.025])\n",
    "compustat['rd_f'] = winsorize(compustat['rd_f'], limits=[0.025, 0.025])\n",
    "compustat['intang_f'] = winsorize(compustat['intang_f'], limits=[0.025, 0.025])\n",
    "compustat['dpt_f'] = winsorize(compustat['dpt_f'], limits=[0.025, 0.025])\n",
    "compustat['mergers'] = winsorize(compustat['mergers'], limits=[0.025, 0.025])\n",
    "compustat['leverage'] = winsorize(compustat['leverage'], limits=[0.025, 0.025])\n",
    "compustat['earnings_volatility'] = winsorize(compustat['earnings_volatility'], limits=[0.025, 0.025])\n",
    "compustat['tobin_q'] = winsorize(compustat['tobin_q'], limits=[0.025, 0.025])\n",
    "\n",
    "\n",
    "# Replace NaN values with 0\n",
    "compustat = compustat.fillna(0)\n",
    "compustat['roe'].describe(percentiles=[.01, .05, .1, .25, .5, .75, .9, .95, .99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25df17a",
   "metadata": {},
   "source": [
    "# Compustat Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_downloaded_compustat_annual = True\n",
    "\n",
    "if previously_downloaded_compustat_annual == False:\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    # download finratiofirm table\n",
    "    query = \"\"\"\n",
    "    SELECT gvkey, datadate, datafmt, popsrc, consol, indfmt, cusip, cik,  sich,  naicsh, sale, ppent, emp, xrd, xad\n",
    "    FROM comp_na_daily_all.funda\n",
    "    WHERE fyear >= 2000\n",
    "    \"\"\"\n",
    "    compustata = db.raw_sql(query)\n",
    "    compustata.to_pickle('../data/compustat_a.pkl')\n",
    "\n",
    "else:\n",
    "    # Load the ratio data from the pickle file\n",
    "    compustata = pd.read_pickle('../data/compustat_a.pkl')\n",
    "    \n",
    "compustata=compustata[compustata.datafmt=='STD']\n",
    "compustata=compustata[compustata.popsrc=='D']\n",
    "compustata=compustata[compustata.consol=='C']\n",
    "compustata=compustata[compustata.indfmt== 'INDL']\n",
    "\n",
    "compustata.drop_duplicates(['gvkey','datadate'], inplace=True)\n",
    "\n",
    "compustata['datadate'] = pd.to_datetime(compustata['datadate'], format='%Y%m%d')\n",
    "\n",
    "compustata['month']=compustata['datadate'].dt.month\n",
    "compustata['year']=compustata['datadate'].dt.year\n",
    "\n",
    "compustata['sich'] = compustata['sich'].replace('', np.nan)\n",
    "compustata = compustata.loc[compustata['sich'].notna()]\n",
    "compustata.loc[:, 'sich4'] = compustata['sich'].astype(int).astype(str).str.pad(width=4, side='right', fillchar='0').astype(int)\n",
    "compustata.loc[:, 'sich3'] = compustata['sich4'] // 10\n",
    "compustata.loc[:, 'sich2'] = compustata['sich4'] // 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c1814",
   "metadata": {},
   "source": [
    "# Uniqueness Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f884769",
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_downloaded_segments = True\n",
    "if previously_downloaded_segments == False:\n",
    "\n",
    "    wrds_username = os.getenv('WRDS_USERNAME')\n",
    "    wrds_password = os.getenv('WRDS_PASSWORD')\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "\n",
    "    # replace with wrdssec.forms\n",
    "    query = f\"\"\"\n",
    "        SELECT gvkey, srcdate, datadate, sid, sales, stype, sics1\n",
    "        FROM comp_segments_hist_daily.wrds_segmerged\n",
    "    \"\"\"\n",
    "    \n",
    "    segments = db.raw_sql(query)\n",
    "    segments.to_csv('../data/compustat_segments.csv')\n",
    "    db.close()\n",
    "    \n",
    "else:\n",
    "    segments = pd.read_csv('../data/compustat_segments.csv')\n",
    "segments.stype.value_counts()\n",
    "segments.drop_duplicates(['gvkey', 'datadate','srcdate'])\n",
    "segments.drop_duplicates(['gvkey', 'datadate', 'sid'])\n",
    "segments = segments[['gvkey', 'datadate', 'sid', 'sales', 'stype']]\n",
    "segments['gvkey'] = segments['gvkey'].astype(int)\n",
    "segments_n = segments[(segments['stype']=='BUSSEG') | (segments['stype']=='OPSEG')].groupby(['gvkey', 'datadate'])['sid'].nunique().reset_index()\n",
    "segments_n = segments_n.rename(columns={'sid': 'n_segments'})\n",
    "\n",
    "segments_n['year'] = segments_n['datadate'].astype(str).str.slice(0,4).astype(int)\n",
    "segments_n = segments_n[['gvkey', 'year', 'n_segments']]\n",
    "\n",
    "compustata['gvkey'] = compustata['gvkey'].astype(int)\n",
    "compustata = compustata.merge(segments_n, on=['gvkey', 'year'], how='left',\n",
    "                                  suffixes=('_df1', ''))\n",
    "compustata['n_segments'].fillna(1,inplace=True)\n",
    "## Strategy Uniqueness using Litov et al. (2012) method\n",
    "seg_ind = 'sics1'\n",
    "\n",
    "segments = pd.read_csv('../data/compustat_segments.csv')\n",
    "segments['year'] = segments['datadate'].str.slice(0,4).astype(int)\n",
    "segments = segments[(segments['stype'] == 'BUSSEG') | (segments['stype'] == 'OPSEG')]\n",
    "segments = segments[['gvkey', 'year', 'sales', seg_ind ]]\n",
    "segments = segments[segments['sales'] > 0]\n",
    "segments = segments[segments[seg_ind].notnull()]\n",
    "segments = segments[segments[seg_ind] != '']\n",
    "segments = segments[segments[seg_ind] != 0]\n",
    "segments[seg_ind] = segments[seg_ind].astype(int)\n",
    "segments['year'] = segments['year'].astype(int)\n",
    "segments['gvkey'] = segments['gvkey'].astype(int)\n",
    "segments = segments.rename(columns={'gvkey': 'GVKEY'})\n",
    "segments = segments.rename(columns={seg_ind: 'segment_sic'})\n",
    "segments = segments.rename(columns={'sales': 'segment_sale'})\n",
    "segments = segments.groupby(['GVKEY', 'year', 'segment_sic'])['segment_sale'].sum().reset_index(name='segment_sale')\n",
    "segments['segment_sic'] = segments['segment_sic'].astype(int)\n",
    "\n",
    "# Step 1\n",
    "idx = segments.groupby(['GVKEY', 'year'])['segment_sale'].idxmax()\n",
    "segments['primary_sic'] = segments.loc[idx, 'segment_sic']\n",
    "segments['primary_sic'] = segments.groupby(['GVKEY', 'year'])['primary_sic'].transform('max')\n",
    "\n",
    "# Step 2\n",
    "total_sales = segments.groupby(['GVKEY', 'year'])['segment_sale'].transform('sum')\n",
    "segments['norm_sale'] = segments['segment_sale'] / total_sales\n",
    "\n",
    "# Step 3\n",
    "firm_year_matrix = segments.pivot_table(index=['GVKEY', 'year', 'primary_sic'],\n",
    "                                        columns='segment_sic',\n",
    "                                        values='norm_sale').fillna(0)\n",
    "\n",
    "# Step 4\n",
    "actual_sales_matrix = segments.pivot_table(index=['GVKEY', 'year', 'primary_sic'],\n",
    "                                           columns='segment_sic',\n",
    "                                           values='segment_sale').fillna(0)\n",
    "\n",
    "industry_year_sales = actual_sales_matrix.groupby(['primary_sic', 'year']).sum()\n",
    "\n",
    "# Step 5\n",
    "total_industry_sales = industry_year_sales.sum(axis=1)\n",
    "norm_industry_year_sales = industry_year_sales.div(total_industry_sales, axis=0)\n",
    "\n",
    "# Step 6\n",
    "diff_matrix = firm_year_matrix.subtract(norm_industry_year_sales, axis=1)\n",
    "\n",
    "# step 7: sum of squared differences\n",
    "squared_diff_matrix = diff_matrix ** 2\n",
    "sum_squared_diff = squared_diff_matrix.sum(axis=1)\n",
    "\n",
    "uniqueness = sum_squared_diff.reset_index(name='strategy_unique')\n",
    "uniqueness = uniqueness.rename(columns={'GVKEY': 'gvkey'})\n",
    "uniqueness.drop_duplicates(['gvkey', 'year'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d52384",
   "metadata": {},
   "source": [
    "# BoardEx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e138eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex = pd.read_stata(\"../data/NA - BoardEx - Organization Summary - Analytics.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5528a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fa7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex = boardex[['CompanyID', 'AnnualReportDate', 'Succession', 'Attrition',\n",
    "       'GenderRatio', 'NationalityMix', 'NumberDirectors', 'STDEVTimeBrd',\n",
    "       'STDEVTimeInCo', 'STDEVTotNoLstdBrd', 'STDEVTotCurrNoLstdBrd',\n",
    "       'STDEVNoQuals', 'STDEVAge', 'NetworkSize',]]\n",
    "\n",
    "# drop duplciates for companyid and annualreportdate\n",
    "boardex = boardex.drop_duplicates(['CompanyID', 'AnnualReportDate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addaa358",
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex_map = pd.read_stata(\"../data/NA - BoardEx_linking_map.dta\")\n",
    "\n",
    "# keep only if preferred ==1\n",
    "boardex_map = boardex_map[boardex_map['preferred'] == 1]\n",
    "\n",
    "# keep only compnayid and gvkey\n",
    "boardex_map = boardex_map[['companyid', 'GVKEY']]\n",
    "\n",
    "# drop duplicates\n",
    "boardex_map = boardex_map.drop_duplicates(['companyid', 'GVKEY'])\n",
    "\n",
    "# rename companyid to CompanyID and GVKEY to gvkey\n",
    "boardex_map = boardex_map.rename(columns={'companyid': 'CompanyID', 'GVKEY': 'gvkey'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge boardex with boardex_map on CompanyID\n",
    "boardex = boardex.merge(boardex_map, left_on='CompanyID', right_on='CompanyID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f78880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep if gvkey is not null\n",
    "boardex = boardex[boardex['gvkey'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69488eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert AnnualReportDate to datetime\n",
    "boardex['AnnualReportDate'] = pd.to_datetime(boardex['AnnualReportDate'])\n",
    "\n",
    "boardex['year_b'] = boardex['AnnualReportDate'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates gvkey and year\n",
    "boardex = boardex.drop_duplicates(['gvkey', 'year_b'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5faf4e7",
   "metadata": {},
   "source": [
    "# Merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf5f6e",
   "metadata": {},
   "source": [
    "## Merge Compustata with uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c443c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gvkey to int\n",
    "uniqueness['gvkey'] = uniqueness['gvkey'].astype(int)\n",
    "compustata['gvkey'] = compustata['gvkey'].astype(int)\n",
    "compustata = compustata.merge(uniqueness, on=['gvkey', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8783c6",
   "metadata": {},
   "source": [
    "## Merge compustat with compustata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata['gvkey'] = compustata['gvkey'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata_merged = compustata.merge(compustat, on=['gvkey', 'datadate'], how='inner', suffixes=('_a', '_q'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff284717",
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata_merged.drop_duplicates(['gvkey','datadate'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fyear column which is year_a if fyr is 6 or larger, otherwise it is year_a-1\n",
    "compustata_merged['fyear'] = np.where(compustata_merged['fyr'] >= 6, compustata_merged['year_a'], compustata_merged['year_a'] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d6513",
   "metadata": {},
   "source": [
    "## Merge boardex and Compustata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge boardex and compustata_merged on gvkey and year (year_b for boardex, year_a for compustata_merged)\n",
    "boardex['year_b'] = boardex['year_b'].astype(int)\n",
    "boardex['gvkey'] = boardex['gvkey'].astype(int)\n",
    "compustata_merged = compustata_merged.merge(boardex, left_on=['gvkey', 'year_a'], right_on=['gvkey', 'year_b'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f699bd4",
   "metadata": {},
   "source": [
    "## Merge execucomp with compustata_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cec1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge execucomp with compustata_merged on gvkey and year = fyear\n",
    "execucomp_merged = execucomp.merge(compustata_merged, left_on=['gvkey', 'year'], right_on=['gvkey', 'fyear'], how='inner', suffixes=('_e', '_c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca735e",
   "metadata": {},
   "source": [
    "## Merge ceo_dismissal with execucomp_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff905dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open /Users/majid/Dropbox/Promises/transcripts_wrds/data/CEO Dismissal Database Posted to Web 9Nov23.xlsx\n",
    "\n",
    "dismissal_data = pd.read_excel(\"../data/CEO Dismissal Database Posted to Web 9Nov23.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge execucomp  [gvkey, year, co_per_rol] with dismissal_data on [gvkey, fyear, co_per_rol]\n",
    "ceo_dismissal = execucomp_merged.merge(dismissal_data, left_on=['gvkey', 'year', 'co_per_rol'], right_on=['gvkey', 'fyear', 'co_per_rol'], how='left', suffixes=('_e', '_d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ec8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var involuntary_dismissal = 1 if departure_code is 3 or 4\n",
    "ceo_dismissal['involuntary_dismissal'] = ceo_dismissal['departure_code'].isin([3, 4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_dismissal['involuntary_dismissal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ceo_dismissal on execid, year with result on execid, year\n",
    "ceo_dismissal = ceo_dismissal.merge(result, on=['execid', 'year'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e934b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to execid s that show in results\n",
    "ceo_dismissal_results = ceo_dismissal[ceo_dismissal['execid'].isin(result['execid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d36a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['execid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_dismissal_results['execid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace no_broken_promises with 0 if missing\n",
    "ceo_dismissal_results['no_broken_promises'] = ceo_dismissal_results['no_broken_promises'].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60339ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_dismissal_results['no_broken_promises'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe no_promises_prior    \n",
    "\n",
    "ceo_dismissal_results['no_promises_prior'] = ceo_dismissal_results['no_promises_prior'].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_dismissal_results['no_promises_prior'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d81cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_dismissal_results['involuntary_dismissal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by gvkey year, keep only duplicates\n",
    "ceo_dismissal_results[ceo_dismissal_results.duplicated(subset=['gvkey', 'year'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5633f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop gvkey, year duplicates\n",
    "ceo_dismissal_results = ceo_dismissal_results.drop_duplicates(subset=['gvkey', 'year'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b777f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ceo_dismissal_results.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to stata - handle infinity values and column name issues\n",
    "ceo_dismissal_results_clean = ceo_dismissal_results.copy()\n",
    "\n",
    "# Replace infinity values with NaN for all numeric columns\n",
    "numeric_cols = ceo_dismissal_results_clean.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    ceo_dismissal_results_clean[col] = ceo_dismissal_results_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Handle object columns that might cause issues\n",
    "object_cols = ceo_dismissal_results_clean.select_dtypes(include=['object']).columns\n",
    "for col in object_cols:\n",
    "    # Convert problematic object columns to string or drop if all null\n",
    "    if ceo_dismissal_results_clean[col].isna().all():\n",
    "        ceo_dismissal_results_clean = ceo_dismissal_results_clean.drop(columns=[col])\n",
    "        print(f\"Dropped column {col} - all null values\")\n",
    "    else:\n",
    "        # Convert to string and handle None values\n",
    "        ceo_dismissal_results_clean[col] = ceo_dismissal_results_clean[col].astype(str)\n",
    "        ceo_dismissal_results_clean[col] = ceo_dismissal_results_clean[col].replace('nan', '')\n",
    "        ceo_dismissal_results_clean[col] = ceo_dismissal_results_clean[col].replace('None', '')\n",
    "        # Handle unicode characters that cause encoding issues\n",
    "        ceo_dismissal_results_clean[col] = ceo_dismissal_results_clean[col].str.encode('ascii', errors='ignore').str.decode('ascii')\n",
    "        \n",
    "        # Truncate strings that are too long for Stata (even v117 has a limit of 2045 chars)\n",
    "        max_length = 2000  # Leave some buffer\n",
    "        if col in ceo_dismissal_results_clean.columns:\n",
    "            ceo_dismissal_results_clean[col] = ceo_dismissal_results_clean[col].str[:max_length]\n",
    "\n",
    "# Fix column names for Stata compatibility (max 32 chars, alphanumeric + underscore only)\n",
    "column_mapping = {}\n",
    "for col in ceo_dismissal_results_clean.columns:\n",
    "    new_col = col\n",
    "    # Remove spaces and special characters\n",
    "    new_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in new_col)\n",
    "    # Truncate to 32 characters\n",
    "    if len(new_col) > 32:\n",
    "        new_col = new_col[:32]\n",
    "    # Ensure it doesn't start with a number\n",
    "    if new_col[0].isdigit():\n",
    "        new_col = 'var_' + new_col[:28]\n",
    "    column_mapping[col] = new_col\n",
    "\n",
    "ceo_dismissal_results_clean = ceo_dismissal_results_clean.rename(columns=column_mapping)\n",
    "\n",
    "# Save to stata with version 117 (Stata 13+) to handle longer strings\n",
    "try:\n",
    "    ceo_dismissal_results_clean.to_stata('../../data/ceo_dismissal_reg_data.dta', \n",
    "                                          write_index=False,\n",
    "                                          version=117)\n",
    "    print(\"Successfully saved to Stata format (version 117)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to Stata: {e}\")\n",
    "    # Alternative: save as CSV\n",
    "    ceo_dismissal_results_clean.to_csv('../../data/ceo_dismissal_reg_data.csv', index=False)\n",
    "    print(\"Saved as CSV instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcfcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique execid\n",
    "ceo_dismissal_results_clean['gvkey'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86989594",
   "metadata": {},
   "source": [
    "# Some Random Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc5cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep if the no_promises_prior_roll5 is above 50\n",
    "batch_all_50 = ceo_ranking_merged[ceo_ranking_merged['no_promises_prior_roll5'] > 50]\n",
    "# create column year in promises_select_10percent_results_merged based on mostimportantdateutc\n",
    "\n",
    "high_promises = promises_select_10percent_results_merged[promises_select_10percent_results_merged['gvkey'] == 25434]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899e272",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcripts_wrds_jupyter2",
   "language": "python",
   "name": "transcripts_wrds_jupyter2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
