{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        handlers=[\n",
    "        logging.FileHandler(\"../data/find_promises_log_20240101.log\"),  # Log messages are written to this file\n",
    "        logging.StreamHandler()  # Log messages are also printed to the console\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/1pq07r555gxgt22gqksl2tlw0000gn/T/ipykernel_95077/2920822325.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  promises = pd.read_csv(\"../data/LIWC-22 Results - sxp1500_presentations_ceo_aggr___ - LIWC Analysis_v11.csv\")\n"
     ]
    }
   ],
   "source": [
    "promises = pd.read_csv(\"../data/sxp1500_presentations_ceo_aggregated_promises_expanded_cleaned_transcriptlevel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "promises['processed'] = 0\n",
    "promises['result'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these values based on your needs\n",
    "CONCURRENT_REQUESTS = 250  # Maximum number of concurrent requests\n",
    "TIMEOUT = 240  # Timeout in seconds for each request\n",
    "\n",
    "# Constants\n",
    "batch_size = 1000  # Number of requests after which to checkpoint\n",
    "save_file = '../data/sxp1500_presentations_ceo_aggregated_promises_expanded_cleaned_transcriptlevel_horizon.pkl'\n",
    "\n",
    "# for sample run\n",
    "#initial_file = '../data/sample_transcripts.pkl'\n",
    "\n",
    "open_ai_api_key=\"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch promise identification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_template = \"\"\"\n",
    "I am analyzing CEO promises from earnings conference calls. For each call, I have the following inputs:\n",
    "• Conference date (in yyyy-mm-dd format)\n",
    "• Stated delivery time or date of the promise\n",
    "\n",
    "From these inputs, I want you to determine the number of months between the conference date and the stated delivery time. If the delivery date is not specified or is too ambiguous to deduce a clear timeframe, return “unclear.”\n",
    "\n",
    "For delivery date ranges (e.g., “within the next few months” or “over the next few quarters”), return the maximum of that range. For example:\n",
    "• “A few months” might be 2–9 months, so return 9.\n",
    "• “Between now and the next three years” might be 0–36 months, so return 36.\n",
    "\n",
    "Your output must be strictly one of the following formats:\n",
    "“unclear”\n",
    "Or\n",
    "\"An integer representing the months to delivery\"ArithmeticError\n",
    "\n",
    "Absolutely no other text or characters should be returned. Don't explain how you arrived at the answer. Just return the answer.\n",
    "\n",
    "\n",
    "\n",
    "Conference date (yyyy-mm-dd): {placeholder_conference_date}\n",
    "Delivery time: {placeholder_delivery_date}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# CORRECTED:\n",
    "def create_user_message(row, user_message_template):\n",
    "    # Ensure data is string type for replacement, handle potential NaNs\n",
    "    delivery_time = str(row[\"3-promise-delivery-time\"]) if pd.notna(row[\"3-promise-delivery-time\"]) else \"\"\n",
    "    conference_date = str(row[\"mostimportantdateutc\"]) if pd.notna(row[\"mostimportantdateutc\"]) else \"\"\n",
    "\n",
    "    user_message = user_message_template.replace(\"{placeholder_conference_date}\", conference_date) \\\n",
    "        .replace(\"{placeholder_delivery_date}\", delivery_time)\n",
    "    return user_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = AsyncOpenAI(api_key=open_ai_api_key)\n",
    "\n",
    "# Semaphore to control the number of concurrent requests\n",
    "semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(multiplier=1, max=10))\n",
    "async def fetch_chat_completion(user_message, index):\n",
    "    async with semaphore:\n",
    "        logging.info(f\"Processing request for row {index}\")\n",
    "        try:\n",
    "            response = await asyncio.wait_for(client.chat.completions.create(\n",
    "                model=\"o3-mini-2025-01-31\",\n",
    "                reasoning_effort=\"medium\",\n",
    "                seed=2025,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_message}]\n",
    "            ), TIMEOUT)\n",
    "            logging.info(f\"Completed request for row {index}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in request for row {index}: {e}\")\n",
    "            return None\n",
    "\n",
    "async def process_rows(df, BATCH_SIZE, save_file):\n",
    "    df_non_processed = df[df['processed'] == 0]\n",
    "\n",
    "    total_prompt_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "\n",
    "    for start_index in range(0, len(df_non_processed), BATCH_SIZE):\n",
    "        end_index = start_index + BATCH_SIZE\n",
    "        batch_df = df_non_processed.iloc[start_index:end_index]\n",
    "        \n",
    "        task_list = [\n",
    "            (index, asyncio.create_task(fetch_chat_completion(create_user_message(row, user_message_template), index)))\n",
    "            for index, row in batch_df.iterrows()\n",
    "        ]\n",
    "        responses = await asyncio.gather(*[task for index, task in task_list], return_exceptions=True)\n",
    "        \n",
    "        for (index, _), response in zip(task_list, responses):\n",
    "            if isinstance(response, Exception):\n",
    "                df.at[index, 'result'] = None\n",
    "            elif response is None or pd.isna(response.choices[0].message.content) or response.choices[0].message.content == '' or response.choices[0].message.content == 'None':\n",
    "                df.at[index, 'result'] = None\n",
    "\n",
    "                try:\n",
    "                    total_prompt_tokens += response.usage.prompt_tokens\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                df.at[index, 'result'] = response.choices[0].message.content\n",
    "                df.at[index, 'processed'] = 1\n",
    "\n",
    "                try:\n",
    "                    total_prompt_tokens += response.usage.prompt_tokens\n",
    "                    total_completion_tokens += response.usage.completion_tokens\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Save intermediate results\n",
    "        df.to_pickle(save_file)\n",
    "        \n",
    "\n",
    "    # Account for responses that are None, NaN, or empty\n",
    "    df['result'] = df['result'].replace('', None).replace(float('nan'), None)\n",
    "    df.to_pickle(save_file)\n",
    "    return df, total_prompt_tokens, total_completion_tokens\n",
    "\n",
    "\n",
    "# Running the asynchronous tasks with checkpointing\n",
    "async def main(batch_size, save_file):\n",
    "    # check if save file exists\n",
    "    \n",
    "    # Process remaining rows\n",
    "    results_df, total_prompt_tokens, total_completion_tokens = await process_rows(promises, batch_size, save_file)\n",
    "    return results_df, total_prompt_tokens, total_completion_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main function in Jupyter notebook\n",
    "results, total_prompt_tokens, total_completion_tokens = await main(batch_size, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results['result'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unclear promise horizons: 46.72%\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage of promise horizons that are \"unclear\"\n",
    "unclear_count = len(results[results['result'] == 'unclear'])\n",
    "total_count = len(results)\n",
    "unclear_percentage = unclear_count / total_count if total_count > 0 else 0\n",
    "print(f\"Percentage of unclear promise horizons: {unclear_percentage:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = promises.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column results to 3-promise-horizon-v2\n",
    "results.rename(columns={'result': '3-promise-horizon-v2'}, inplace=True)\n",
    "results.to_csv(\"../data/sxp1500_presentations_ceo_aggregated_promises_expanded_cleaned_transcriptlevel_horizon.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcripts_wrds_jupyter2",
   "language": "python",
   "name": "transcripts_wrds_jupyter2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
