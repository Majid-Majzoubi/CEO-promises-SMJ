{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Set Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "1- Import all the libraries\n",
    "2- Download the Summary History - Summary Statistics: earnings suprises in past quarters, number of analysts, etc.\n",
    "3- Compustat quarterly variables: firm performance, size, leverage, profitability, liquidity, growth, age\n",
    "4- Industry level variables: HHI, R&D, etc.\n",
    "5- Execucomp variables: CEO characteristics, overconfidence, age, tenure, gender, education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wrds\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_downloaded_ratios = True\n",
    "previously_downloaded_age = True\n",
    "previously_downloaded_compustat_quarterly = True\n",
    "previously_downloaded_compustat_annual = True\n",
    "previously_downloaded_segments = True\n",
    "previously_downloaded_estimates = True\n",
    "previously_downloaded_execucomp = True\n",
    "previously_downloaded_Loughran_McDonald = True\n",
    "ind_col = 'sich4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access environment variables for WRDS username and password\n",
    "wrds_username = os.environ.get(\"WRDS_USERNAME\")\n",
    "wrds_password = os.environ.get(\"WRDS_PASSWORD\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Base dataframe: transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = pd.read_pickle(\"../data/sxp1500_presentations_ceo_aggregated.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts.gvkey.nunique()\n",
    "transcripts.columns\n",
    "# check to see if there are duplicate transcriptid values per keydevid\n",
    "transcripts.groupby(['keydevid'])['transcriptid'].apply(lambda x: x.nunique()>1).sum()\n",
    "# number of words in transcript_text\n",
    "transcripts['word_count_total'] = transcripts['transcript_text'].str.split().str.len()\n",
    "transcripts = transcripts.drop_duplicates(subset=['keydevid'], keep='first')\n",
    "database = transcripts[['companyid', 'keydevid', 'transcriptid', 'mostimportantdateutc', 'mostimportanttimeutc', 'gvkey', 'companyname', 'word_count_total']]\n",
    "database['mostimportantdateutc_dt'] = pd.to_datetime(database['mostimportantdateutc'], format='%Y-%m-%d')\n",
    "#database['year'] = database['mostimportantdateutc_dt'].dt.year\n",
    "#database['month'] = database['mostimportantdateutc_dt'].dt.month\n",
    "# 2 - Promises identified\n",
    "promises = pd.read_csv(\"../data/sxp1500_presentations_ceo_aggregated_promises_expanded_cleaned_transcriptlevel_horizon_specificity.csv\")\n",
    "# create promise_id column, it is gvkey_transcriptid_2digitnumber (01, 02, 03, ...)\n",
    "\n",
    "promises['promise_id'] = promises.groupby(['gvkey', 'transcriptid']).cumcount() + 1\n",
    "promises['promise_id'] = promises['gvkey'].astype(str) + '_' + promises['transcriptid'].astype(str) + '_' + promises['promise_id'].apply(lambda x: f'{x:02d}')\n",
    "\n",
    "\n",
    "list(promises.columns)\n",
    "promises['3-promise-horizon-v2'].value_counts()\n",
    "### Cleaning up the horizons column\n",
    "# if it contains 'unclear' or \"Unclear\" in the promise, then set the promise horizon to 'unclear'\n",
    "promises.loc[promises['3-promise-horizon-v2'].str.contains('unclear', case=False, na=False), '3-promise-horizon-v2'] = 'unclear'\n",
    "\n",
    "def process_value(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    if value == 'unclear':\n",
    "        return np.nan\n",
    "    if not str(value).replace('-', '').replace('.', '').isdigit():\n",
    "        return np.nan\n",
    "    if '-' in value:\n",
    "        try:\n",
    "            number1, number2 = value.split('-')\n",
    "            return (float(number1) + float(number2)) / 2\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "promises['promise_horizon_months'] = promises['3-promise-horizon-v2'].apply(process_value)\n",
    "\n",
    "promises['promise_horizon_months'].describe()\n",
    "### Promises count\n",
    "def process_promises(filtered_promises, suffix):\n",
    "    # promises count\n",
    "    filtered_promises[f'promises{suffix}_count'] = filtered_promises.groupby(['transcriptid'])['promise_id'].transform('nunique')\n",
    "\n",
    "    # promises deliver date averaging\n",
    "    filtered_promises[f'promises{suffix}_horizon'] = filtered_promises.groupby(['transcriptid'])[f'promise_horizon_months'].transform(lambda x: x.mean(skipna=True))\n",
    "\n",
    "    # proportion of horizons that are nan per transcript id\n",
    "    filtered_promises[f'promises{suffix}_horizon_nan'] = filtered_promises.groupby(['transcriptid'])['promise_horizon_months'].transform(lambda x: x.isna().sum()/len(x))\n",
    "    \n",
    "    filtered_promises[f'promises{suffix}_specificity_score'] = filtered_promises.groupby(['transcriptid'])['specificity_score'].transform(lambda x: x.mean(skipna=True))\n",
    "    \n",
    "    # Keeping relevant columns\n",
    "    promise_columns = [column for column in filtered_promises.columns if f'promises{suffix}_' in column]\n",
    "    promise_columns_keep = ['transcriptid']\n",
    "    promise_columns_keep.extend(promise_columns)\n",
    "\n",
    "    return filtered_promises[promise_columns_keep]\n",
    "\n",
    "promises1 = promises[((promises['7-is-promise'] == 'yes') | (promises['7-is-promise'] == 'Yes')) &\n",
    "                    ((promises['8-financial-guidance'] == 'no') | (promises['8-financial-guidance'] == 'No')) & \n",
    "                    ((promises['5-commitment-degree'] == 'strong-commitment') )]\n",
    "\n",
    "promises1 = process_promises(promises1, '_1')\n",
    "\n",
    "promises1.drop_duplicates(subset=['transcriptid'], keep='first', inplace=True)\n",
    "\n",
    "database = pd.merge(database, promises1, on='transcriptid', how='left')\n",
    "database['promises_1_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access environment variables for WRDS username and password\n",
    "wrds_username = os.environ.get(\"WRDS_USERNAME\")\n",
    "wrds_password = os.environ.get(\"WRDS_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ratio data from WRDS\n",
    "if previously_downloaded_ratios == False:\n",
    "    # Connect to WRDS using the provided username and password\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    # download finratiofirm table\n",
    "    ratios = db.get_table(library='wrdsapps_finratio_ibes', table='firm_ratio_ibes')\n",
    "    # convert adate column to datetime, get year\n",
    "    ratios['adate_dt'] = pd.to_datetime(ratios['adate'], format='%Y%m%d')\n",
    "    ratios['year'] = ratios['adate_dt'].dt.year\n",
    "    ratios = ratios[ratios['year'] >= 2005]\n",
    "    ratios.to_pickle('../data/ratios.pkl')\n",
    "else:\n",
    "    # Load the ratio data from the pickle file\n",
    "    ratios = pd.read_pickle('../data/ratios.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ratios[['gvkey', 'adate', 'qdate', 'roe']].head(10000)\n",
    "ratios.gvkey.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio_columns = ['gvkey', 'adate', 'qdate', 'bm', 'pe_inc', 'pe_exi', 'ptb', 'npm', 'roa', 'roe', 'roce', 'cfm', 'debt_at', 'at_turn', 'rd_sale', 'adv_sale']\n",
    "#ratios = ratios[ratio_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios.sort_values(by=['gvkey', 'adate', 'qdate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios.drop_duplicates(subset=['gvkey', 'adate', 'qdate'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a ratio_ prefix to each column name except gvkey adate and qdate\n",
    "ratios.columns = ['ratio_' + column if column not in ['gvkey', 'adate', 'qdate'] else column for column in ratios.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldate is the last date between adate and qdate\n",
    "ratios['ldate'] = ratios[['adate', 'qdate']].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earnings volatility: standard deviation of roa over the last 12 quarters\n",
    "ratios['ratio_earnings_volatility'] = ratios.groupby(['gvkey'])['ratio_roa'].transform(lambda x: x.rolling(6).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios['ratio_earnings_volatility'].notna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Compustat Quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_downloaded_compustat_quarterly = True\n",
    "if previously_downloaded_compustat_quarterly == False:\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    # download finratiofirm table\n",
    "    query = \"\"\"\n",
    "    SELECT gvkey, datadate, fyearq, fqtr,indfmt, consol, popsrc, datafmt, fyr, actq, atq, ibq, niq, aqcy, niy, epsfi12, oeps12, epsfxy, mkvaltq, prccq, prchq, prclq, saleq, cshoq, actq, lctq, xoprq, xrdq, intanq, txdbq, dpq, aqpq, dlttq, dlcq, seqq\n",
    "    FROM comp_na_daily_all.fundq\n",
    "    WHERE fyearq >= 2003\n",
    "    \"\"\"\n",
    "    compustat = db.raw_sql(query)\n",
    "    compustat.to_pickle('../data/compustat_q_2.pkl')\n",
    "\n",
    "else:\n",
    "    # Load the ratio data from the pickle file\n",
    "    compustat = pd.read_pickle('../data/compustat_q_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat=compustat[compustat.datafmt=='STD']\n",
    "compustat=compustat[compustat.popsrc=='D']\n",
    "compustat=compustat[compustat.consol=='C']\n",
    "compustat=compustat[compustat.indfmt== 'INDL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compustat['datadate'] = pd.to_datetime(compustat['datadate'], format='%Y-%m-%d')\n",
    "compustat['month']=compustat['datadate'].dt.month\n",
    "compustat['year']=compustat['datadate'].dt.year\n",
    "compustat.drop_duplicates(['gvkey','datadate'], inplace=True)\n",
    "compustat['gvkey'] = compustat['gvkey'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPA\n",
    "compustat['EPS'] = np.where(compustat['cshoq'] != 0, compustat['niq'] / compustat['cshoq'], np.nan)\n",
    "# roa\n",
    "compustat['roa'] = np.where(compustat['atq'] != 0, compustat['niq'] / compustat['atq'], np.nan)\n",
    "\n",
    "# r&d share of total expense\n",
    "compustat['rd_f'] = compustat['xrdq'] / compustat['xoprq'].where(compustat['xoprq'] != 0, np.nan)\n",
    "compustat['rd_f'] = compustat['rd_f'].fillna(0)\n",
    "\n",
    "# recognized intangible assets as part of total assets\n",
    "compustat['intang_f'] = compustat['intanq'] / compustat['atq'].where(compustat['atq'] != 0, np.nan)\n",
    "compustat['intang_f'] = compustat['intang_f'].fillna(0)\n",
    "\n",
    "# depreciation as part of total assets\n",
    "compustat['dpt_f'] = compustat['dpq'] / compustat['atq'].where(compustat['atq'] != 0, np.nan)\n",
    "compustat['dpt_f'] = compustat['dpt_f'].fillna(0)\n",
    "\n",
    "# Leverage\n",
    "compustat['leverage'] = (compustat['dlttq'] + compustat['dlcq']) / compustat['seqq']\n",
    "compustat.loc[compustat['seqq'] == 0, 'leverage'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat = compustat.sort_values(['gvkey', 'fyearq', 'fqtr'])\n",
    "compustat['mna_cash_q'] = compustat.groupby(\n",
    "    ['gvkey', 'fyearq']\n",
    ")['aqcy'].diff()\n",
    "\n",
    "# First fiscal quarter\n",
    "compustat['mna_cash_q'] = compustat['mna_cash_q'].fillna(compustat['aqcy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat['mna_cash_q_f'] = np.where(\n",
    "    compustat['atq'] > 0,\n",
    "    compustat['mna_cash_q'] / compustat['atq'],\n",
    "    np.nan\n",
    ")\n",
    "compustat['mna_cash_q_f'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "compustat.loc[compustat['mna_cash_q_f'] < 0, 'mna_cash_q_f'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(compustat['mna_cash_q_f'] < 0).sum()        # should be 0 or very close\n",
    "compustat['mna_cash_q_f'].quantile(0.99)    # should be < ~0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2.1: Cash flow (CF) = ibq + dpq\n",
    "compustat['cf'] = compustat['ibq'] + compustat['dpq']\n",
    "\n",
    "\n",
    "# 2.2: Market Value of Equity (MVE) = cshoq * prccq\n",
    "compustat['mve'] = compustat['cshoq'] * compustat['prccq']\n",
    "\n",
    "# 2.3: Book Debt \n",
    "#      A common approximation is atq - seqq - txdbq.\n",
    "compustat['book_debt'] = compustat['atq'] - compustat['seqq']\n",
    "compustat['book_debt'] = compustat['book_debt'] - compustat['txdbq'].fillna(0)\n",
    "\n",
    "# 2.4: Tobin's Q = (MVE + Book Debt) / atq\n",
    "compustat['tobin_q'] = (compustat['mve'] + compustat['book_debt']) / compustat['atq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earnings_volatility\n",
    "compustat.sort_values(['gvkey', 'datadate'], inplace=True)\n",
    "compustat['earnings_volatility'] = compustat.groupby(['gvkey'])['roa'].transform(lambda x: x.rolling(6).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsorize the variables at the 1% level\n",
    "compustat['EPS'] = winsorize(compustat['EPS'], limits=[0.025, 0.025])\n",
    "compustat['roa'] = winsorize(compustat['roa'], limits=[0.025, 0.025])\n",
    "compustat['rd_f'] = winsorize(compustat['rd_f'], limits=[0.025, 0.025])\n",
    "compustat['intang_f'] = winsorize(compustat['intang_f'], limits=[0.025, 0.025])\n",
    "compustat['dpt_f'] = winsorize(compustat['dpt_f'], limits=[0.025, 0.025])\n",
    "compustat['mna_cash_q_f'] = winsorize(compustat['mna_cash_q_f'], limits=[0.025, 0.025])\n",
    "compustat['leverage'] = winsorize(compustat['leverage'], limits=[0.025, 0.025])\n",
    "compustat['earnings_volatility'] = winsorize(compustat['earnings_volatility'], limits=[0.025, 0.025])\n",
    "compustat['tobin_q'] = winsorize(compustat['tobin_q'], limits=[0.025, 0.025])\n",
    "\n",
    "\n",
    "# Replace NaN values with 0\n",
    "compustat = compustat.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "database_temp = database[['transcriptid', 'gvkey', 'mostimportantdateutc']].merge(compustat, how='left', on='gvkey')\n",
    "database_temp['mostimportantdateutc_dt'] = pd.to_datetime(database_temp['mostimportantdateutc'], format='%Y-%m-%d')\n",
    "database_temp['qdate_diff'] = database_temp['mostimportantdateutc_dt'] - database_temp['datadate']\n",
    "database_temp['qdate_diff'] = database_temp['qdate_diff'].dt.days\n",
    "database_temp = database_temp[(database_temp['qdate_diff'] >= 0) & (database_temp['qdate_diff'] <= 75)]\n",
    "database_temp.drop_duplicates(subset=['transcriptid'], inplace=True)\n",
    "database_temp.drop(columns=['mostimportantdateutc_dt', 'qdate_diff','year', 'month'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_temp.drop_duplicates(subset= ['transcriptid'],inplace=True)\n",
    "database_temp.drop(columns=['gvkey', 'mostimportantdateutc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = database.merge(database_temp, how='left', on='transcriptid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.drop_duplicates(subset=['gvkey', 'datadate'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Compustat annual - industry codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_downloaded_compustat_annual = True\n",
    "\n",
    "if previously_downloaded_compustat_annual == False:\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    # download finratiofirm table\n",
    "    query = \"\"\"\n",
    "    SELECT gvkey, datadate, datafmt, popsrc, consol, indfmt, cusip, cik,  sich,  naicsh, sale, ppent, emp, xrd, xad\n",
    "    FROM comp_na_daily_all.funda\n",
    "    WHERE fyear >= 2000\n",
    "    \"\"\"\n",
    "    compustata = db.raw_sql(query)\n",
    "    compustata.to_pickle('../data/compustat_a.pkl')\n",
    "\n",
    "else:\n",
    "    # Load the ratio data from the pickle file\n",
    "    compustata = pd.read_pickle('../data/compustat_a.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata=compustata[compustata.datafmt=='STD']\n",
    "compustata=compustata[compustata.popsrc=='D']\n",
    "compustata=compustata[compustata.consol=='C']\n",
    "compustata=compustata[compustata.indfmt== 'INDL']\n",
    "\n",
    "compustata.drop_duplicates(['gvkey','datadate'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata['datadate'] = pd.to_datetime(compustata['datadate'], format='%Y-%m-%d')\n",
    "\n",
    "compustata['month']=compustata['datadate'].dt.month\n",
    "compustata['year']=compustata['datadate'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata['sich'] = compustata['sich'].replace('', np.nan)\n",
    "compustata = compustata.loc[compustata['sich'].notna()]\n",
    "compustata.loc[:, 'sich4'] = compustata['sich'].astype(int).astype(str).str.pad(width=4, side='right', fillchar='0').astype(int)\n",
    "compustata.loc[:, 'sich3'] = compustata['sich4'] // 10\n",
    "compustata.loc[:, 'sich2'] = compustata['sich4'] // 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Create industry vars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata['sale_nonneg'] = compustata['sale'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = compustata.groupby(['year', ind_col])['sale_nonneg'].sum().reset_index(name=f'total_sales_{ind_col}')\n",
    "compustata = compustata.merge(total_sales, on=['year', ind_col])\n",
    "compustata[f'market_share_{ind_col}'] = compustata['sale_nonneg'] / compustata[f'total_sales_{ind_col}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata[f'squared_market_share_{ind_col}'] = compustata[f'market_share_{ind_col}'] ** 2\n",
    "hhi = compustata.groupby(['year', ind_col])[f'squared_market_share_{ind_col}'].sum().reset_index(name=f'hhi_{ind_col}')\n",
    "compustata = compustata.merge(hhi, on=['year', ind_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata[f'hhi_{ind_col}'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reverse HHI reversed value = (Max + Min) - original value\n",
    "compustata[f'hhi_{ind_col}_reverse_ind'] = (compustata[f'hhi_{ind_col}'].max() + compustata[f'hhi_{ind_col}'].min()) - compustata[f'hhi_{ind_col}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata[f'hhi_{ind_col}_reverse_ind'].notna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R&D intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### R&D intensity\n",
    "compustata['rd_intensity'] = compustata['xrd'] / compustata['sale']\n",
    "compustata['rd_intensity'] = compustata['rd_intensity'].fillna(0)\n",
    "\n",
    "# industry average per year\n",
    "compustata['rd_intensity_ind'] = compustata.groupby(['year', ind_col])['rd_intensity'].transform(lambda x: x.mean(skipna=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - compustat segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if previously_downloaded_segments == False:\n",
    "\n",
    "    wrds_username = os.getenv('WRDS_USERNAME')\n",
    "    wrds_password = os.getenv('WRDS_PASSWORD')\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "\n",
    "    # replace with wrdssec.forms\n",
    "    query = f\"\"\"\n",
    "        SELECT gvkey, srcdate, datadate, sid, sales, stype, sics1\n",
    "        FROM comp_segments_hist_daily.wrds_segmerged\n",
    "    \"\"\"\n",
    "    \n",
    "    segments = db.raw_sql(query)\n",
    "    segments.to_csv('../data/compustat_segments.csv')\n",
    "    db.close()\n",
    "    \n",
    "else:\n",
    "    segments = pd.read_csv('../data/compustat_segments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.stype.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of segments for each firm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.drop_duplicates(['gvkey', 'datadate','sid'], inplace=True)\n",
    "segments = segments[['gvkey', 'datadate', 'sid', 'sales', 'stype']]\n",
    "segments['gvkey'] = segments['gvkey'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_n = segments[(segments['stype']=='BUSSEG') | (segments['stype']=='OPSEG')].groupby(['gvkey', 'datadate'])['sid'].nunique().reset_index()\n",
    "segments_n = segments_n.rename(columns={'sid': 'n_segments'})\n",
    "\n",
    "segments_n['year'] = segments_n['datadate'].astype(str).str.slice(0,4).astype(int)\n",
    "segments_n = segments_n[['gvkey', 'year', 'n_segments']]\n",
    "\n",
    "compustata['gvkey'] = compustata['gvkey'].astype(int)\n",
    "compustata = compustata.merge(segments_n, on=['gvkey', 'year'], how='left',\n",
    "                                  suffixes=('_df1', ''))\n",
    "compustata['n_segments'].fillna(1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe compustata['n_segments']\n",
    "print(compustata['n_segments'].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Uniqueness using Litov et al. (2012) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_ind = 'sics1'\n",
    "\n",
    "segments = pd.read_csv('../data/compustat_segments.csv')\n",
    "segments['year'] = segments['datadate'].str.slice(0,4).astype(int)\n",
    "segments.drop_duplicates(['gvkey', 'datadate','sid'], inplace=True)\n",
    "segments = segments[(segments['stype'] == 'BUSSEG') | (segments['stype'] == 'OPSEG')]\n",
    "segments = segments[['gvkey', 'year', 'sales', seg_ind ]]\n",
    "segments = segments[segments['sales'] > 0]\n",
    "segments = segments[segments[seg_ind].notnull()]\n",
    "segments = segments[segments[seg_ind] != '']\n",
    "segments = segments[segments[seg_ind] != 0]\n",
    "segments[seg_ind] = segments[seg_ind].astype(int)\n",
    "segments['year'] = segments['year'].astype(int)\n",
    "segments['gvkey'] = segments['gvkey'].astype(int)\n",
    "segments = segments.rename(columns={'gvkey': 'GVKEY'})\n",
    "segments = segments.rename(columns={seg_ind: 'segment_sic'})\n",
    "segments = segments.rename(columns={'sales': 'segment_sale'})\n",
    "segments = segments.groupby(['GVKEY', 'year', 'segment_sic'])['segment_sale'].sum().reset_index(name='segment_sale')\n",
    "segments['segment_sic'] = segments['segment_sic'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1\n",
    "idx = segments.groupby(['GVKEY', 'year'])['segment_sale'].idxmax()\n",
    "segments['primary_sic'] = segments.loc[idx, 'segment_sic']\n",
    "segments['primary_sic'] = segments.groupby(['GVKEY', 'year'])['primary_sic'].transform('max')\n",
    "\n",
    "# Step 2\n",
    "total_sales = segments.groupby(['GVKEY', 'year'])['segment_sale'].transform('sum')\n",
    "segments['norm_sale'] = segments['segment_sale'] / total_sales\n",
    "\n",
    "# Step 3\n",
    "firm_year_matrix = segments.pivot_table(index=['GVKEY', 'year', 'primary_sic'],\n",
    "                                        columns='segment_sic',\n",
    "                                        values='norm_sale').fillna(0)\n",
    "\n",
    "# Step 4\n",
    "actual_sales_matrix = segments.pivot_table(index=['GVKEY', 'year', 'primary_sic'],\n",
    "                                           columns='segment_sic',\n",
    "                                           values='segment_sale').fillna(0)\n",
    "\n",
    "industry_year_sales = actual_sales_matrix.groupby(['primary_sic', 'year']).sum()\n",
    "\n",
    "# Step 5\n",
    "total_industry_sales = industry_year_sales.sum(axis=1)\n",
    "norm_industry_year_sales = industry_year_sales.div(total_industry_sales, axis=0)\n",
    "\n",
    "# Step 6\n",
    "diff_matrix = firm_year_matrix.subtract(norm_industry_year_sales, axis=1)\n",
    "\n",
    "# step 7: sum of squared differences\n",
    "squared_diff_matrix = diff_matrix ** 2\n",
    "sum_squared_diff = squared_diff_matrix.sum(axis=1)\n",
    "\n",
    "uniqueness = sum_squared_diff.reset_index(name='strategy_unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness = uniqueness.rename(columns={'GVKEY': 'gvkey'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness.drop_duplicates(['gvkey', 'year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gvkey to int\n",
    "uniqueness['gvkey'] = uniqueness['gvkey'].astype(int)\n",
    "compustata['gvkey'] = compustata['gvkey'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata = compustata.merge(uniqueness, on=['gvkey', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ratio data from WRDS\n",
    "if previously_downloaded_age == False:\n",
    "    # Connect to WRDS using the provided username and password\n",
    "    db = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    # download finratiofirm table\n",
    "    query = \"\"\"\n",
    "    SELECT gvkey, datadate \n",
    "    FROM comp_na_daily_all.funda\n",
    "    \"\"\"\n",
    "    age = db.raw_sql(query)\n",
    "    age.to_pickle('../data/compustat_age.pkl')\n",
    "\n",
    "else:\n",
    "    # Load the ratio data from the pickle file\n",
    "    age = pd.read_pickle('../data/compustat_age.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age.drop_duplicates(inplace=True)\n",
    "age['datadate_dt'] = pd.to_datetime(age['datadate'], format='%Y-%m-%d')\n",
    "age['year'] = age['datadate_dt'].dt.year\n",
    "age['first_year'] = age.groupby('gvkey')['year'].transform('min')\n",
    "age['firm_age'] = age['year'] - age['first_year']\n",
    "age = age[['gvkey', 'datadate', 'firm_age']]\n",
    "age['gvkey'] = age['gvkey'].astype(int)\n",
    "age.drop_duplicates(subset=['gvkey', 'datadate'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age['datadate'] = pd.to_datetime(age['datadate'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustata = compustata.merge(age, how='left', on=['gvkey', 'datadate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year and month from compustata\n",
    "compustata.drop(columns=['year', 'month'], inplace=True)\n",
    "compustata['gvkey'] = compustata['gvkey'].astype(int)\n",
    "compustata.rename(columns={'datadate': 'datadate_annual'}, inplace=True)\n",
    "database_select = database[['gvkey', 'datadate']]\n",
    "database_select_merge = pd.merge(database_select, compustata, how='outer', on=['gvkey'])\n",
    "database_select_merge.sort_values(['gvkey', 'datadate_annual', 'datadate'], inplace=True)\n",
    "database_select_merge['datadate_diff'] = database_select_merge['datadate'] - database_select_merge['datadate_annual']\n",
    "database_select_merge['datadate_diff'] = database_select_merge['datadate_diff'].dt.days\n",
    "database_select_merge = database_select_merge[(database_select_merge['datadate_diff'] >= 0) & (database_select_merge['datadate_diff'] <= 360)]\n",
    "database_select_merge.drop_duplicates(subset=['gvkey', 'datadate'], inplace=True)\n",
    "database = database.merge(database_select_merge, how='left', on=['gvkey', 'datadate'])\n",
    "database.drop_duplicates(subset=['gvkey', 'datadate'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - IBES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "previously_downloaded_estimates = True\n",
    "if previously_downloaded_estimates == False:\n",
    "\n",
    "    wrds_username = os.getenv('WRDS_USERNAME')\n",
    "    wrds_password = os.getenv('WRDS_PASSWORD')\n",
    "    db_wrds = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "\n",
    "    # replace with wrdssec.forms\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM tr_ibes.statsum_epsus\n",
    "    \"\"\"\n",
    "\n",
    "    estimates = db_wrds.raw_sql(query)\n",
    "    estimates.to_csv('../data/estimates_summary.csv')\n",
    "    db_wrds.close()\n",
    "\n",
    "else:\n",
    "    estimates = pd.read_csv('../data/estimates_summary.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = estimates[(estimates['fiscalp']=='QTR')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = estimates.dropna(subset=['fpedats'])\n",
    "estimates = estimates[estimates['fpedats'] != 'None']\n",
    "estimates['year'] = estimates['fpedats'].astype(str).str.slice(0,4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = estimates.dropna(subset=['actual'])\n",
    "estimates = estimates.dropna(subset=['meanest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = estimates[estimates['year']>2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates['earnings_surprise'] = (estimates['actual'] - estimates['meanest'])\n",
    "estimates['earnings_surprise_norm'] = (estimates['actual'] - estimates['meanest']) / estimates['meanest']\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "estimates = estimates.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates['fpedats_dt'] = pd.to_datetime(estimates['fpedats'], format='%Y-%m-%d')\n",
    "estimates['est_year'] = estimates['fpedats_dt'].dt.year\n",
    "estimates['est_month'] = estimates['fpedats_dt'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates.drop(columns='year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=estimates.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=estimates.head(10000)[['ticker', 'statpers', 'fpedats', 'actdats_act', 'actual', 'meanest', 'earnings_surprise', 'earnings_surprise_norm', 'est_year', 'est_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = estimates.sort_values(['ticker', 'fpedats', 'statpers']).drop_duplicates(['ticker', 'fpedats'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['cusip'] = database['cusip'].str[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_select = database[['cusip', 'datadate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_select.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_select = database_select.merge(estimates, how='outer', on=['cusip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_select['datadate_diff'] = database_select['datadate'] - database_select['fpedats_dt']\n",
    "database_select['datadate_diff'] = database_select['datadate_diff'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_select = database_select[(database_select['datadate_diff'] >= -15) & (database_select['datadate_diff'] <= 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_select['datadate_diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = database.merge(database_select, how='left', on=['cusip', 'datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_select['meanest'].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Execucomp data and CEO compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_downloaded_execucomp = True\n",
    "if previously_downloaded_execucomp == False:\n",
    "        \n",
    "        wrds_username = os.getenv('WRDS_USERNAME')\n",
    "        wrds_password = os.getenv('WRDS_PASSWORD')\n",
    "        db_wrds = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    \n",
    "        # replace with wrdssec.forms\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM  comp_execucomp.anncomp\n",
    "            WHERE ceoann='CEO'\n",
    "        \"\"\"\n",
    "        \n",
    "        execucomp = db_wrds.raw_sql(query)\n",
    "        execucomp['gvkey'] = execucomp['gvkey'].astype(int)\n",
    "\n",
    "        execucomp.to_csv('../data/execucomp.csv')\n",
    "        db_wrds.close()\n",
    "\n",
    "else:\n",
    "    execucomp = pd.read_csv('../data/execucomp.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ceo duality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether the title collumn contains chmn or chairman\n",
    "execucomp['ceo_dual'] = execucomp['title'].str.contains('chmn|chairman', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_dual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp[execucomp['ceo_dual']==1][['title']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_director'] = execucomp['execdir']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### options compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_options_compensation'] = execucomp['opt_unex_exer_est_val']/execucomp['salary']\n",
    "execucomp['ceo_options_compensation_log'] = np.log(execucomp['ceo_options_compensation'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### option awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_option_awards'] = execucomp['option_awards_blk_value']/execucomp['salary']\n",
    "execucomp['ceo_option_awards_log'] = np.log(execucomp['ceo_option_awards'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ceo gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_gender'] = execucomp['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender dummy\n",
    "execucomp['ceo_gender_dummy'] = execucomp['ceo_gender'].apply(lambda x: 1 if x=='MALE' else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ceo age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_age'] = execucomp['age']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ceo tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure your DataFrame is sorted\n",
    "execucomp = execucomp.sort_values(by=['gvkey', 'execid', 'year'])\n",
    "\n",
    "# Create a group identifier for consecutive years\n",
    "execucomp['year_group'] = execucomp.groupby(['gvkey', 'execid'])['year'].diff().ne(1).cumsum()\n",
    "\n",
    "# Create the ceo_tenure variable\n",
    "execucomp['ceo_tenure'] = execucomp.groupby(['gvkey', 'execid', 'year_group']).cumcount() + 1\n",
    "\n",
    "# Drop the temporary 'year_group' column\n",
    "execucomp.drop(columns=['year_group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = execucomp[['gvkey', 'execid', 'year', 'ceo_tenure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_total_compensation'] = execucomp['salary'] + execucomp['bonus'] + execucomp['othcomp']\n",
    "execucomp['ceo_total_compensation_log'] = np.log(execucomp['ceo_total_compensation'])\n",
    "\n",
    "execucomp['ceo_total_deferred_compensation'] = execucomp['defer_balance_tot'] + execucomp['defer_contrib_co_tot'] + execucomp['defer_contrib_exec_tot']\n",
    "execucomp['ceo_total_deferred_compensation_log'] = np.log(execucomp['ceo_total_deferred_compensation'])\n",
    "\n",
    "execucomp['ceo_total_shares_owned'] = execucomp['shrown_tot']\n",
    "execucomp['ceo_total_shares_owned_log'] = np.log(execucomp['ceo_total_shares_owned'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_years_left'] = execucomp.groupby(['gvkey', 'execid'])['year'].transform('max') - execucomp['year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp['ceo_years_left'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['year'] = database['datadate'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp_select = execucomp[['exec_fullname', 'gvkey', 'execid', 'year', 'title', 'ceo_dual', 'ceo_director', 'ceo_options_compensation', 'ceo_options_compensation_log', 'ceo_option_awards', 'ceo_option_awards_log', 'ceo_gender', 'ceo_gender_dummy', 'ceo_age', 'ceo_tenure', 'ceo_total_compensation', 'ceo_total_compensation_log', 'ceo_total_deferred_compensation', 'ceo_total_deferred_compensation_log', 'ceo_total_shares_owned', 'ceo_total_shares_owned_log', 'ceo_years_left'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp_select.drop_duplicates(subset=['gvkey', 'year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = database.merge(execucomp_select, how='left', on=['gvkey', 'year'], suffixes=('', '_excomp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Execucomp - CEO Functional Backgorund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_downloaded_execucomp = True\n",
    "if previously_downloaded_execucomp == False:\n",
    "        \n",
    "        wrds_username = os.getenv('WRDS_USERNAME')\n",
    "        wrds_password = os.getenv('WRDS_PASSWORD')\n",
    "        db_wrds = wrds.Connection(wrds_username=wrds_username, wrds_password=wrds_password)\n",
    "    \n",
    "        # replace with wrdssec.forms\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM  comp_execucomp.anncomp\n",
    "            WHERE ceoann='CEO'\n",
    "        \"\"\"\n",
    "        \n",
    "        execucomp = db_wrds.raw_sql(query)\n",
    "        execucomp['gvkey'] = execucomp['gvkey'].astype(int)\n",
    "\n",
    "        execucomp.to_csv('../data/execucomp.csv')\n",
    "        db_wrds.close()\n",
    "\n",
    "else:\n",
    "    execucomp = pd.read_csv('../data/execucomp.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execucomp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex = pd.read_stata('../data/NA - BoardEx - Organization - Composition of Officers, Directors and Senior Managers.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert DateEndRole and DateStartRole to datetime - format is YYYY-MM-DD\n",
    "boardex['DateEndRole'] = pd.to_datetime(boardex['DateEndRole'], format='%Y-%m-%d')\n",
    "boardex['DateStartRole'] = pd.to_datetime(boardex['DateStartRole'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoleStartYear is the year when the role started\n",
    "# if month is greater than 6, then it is the next year\n",
    "boardex['RoleStartYear'] = boardex['DateStartRole'].dt.year\n",
    "boardex['RoleEndYear'] = boardex['DateEndRole'].dt.year\n",
    "\n",
    "# if month is greater than 6, then it is the next year\n",
    "boardex['RoleStartYear'] = np.where(boardex['DateStartRole'].dt.month > 6, boardex['DateStartRole'].dt.year + 1, boardex['DateStartRole'].dt.year)\n",
    "\n",
    "boardex['RoleEndYear'] = np.where(boardex['DateEndRole'].dt.month > 6, boardex['DateEndRole'].dt.year + 1, boardex['DateEndRole'].dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if roleendyera is empty, replace with 2024\n",
    "boardex['RoleEndYear'] = np.where(boardex['RoleEndYear'].isna(), 2024, boardex['RoleEndYear'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex['RoleLength'] = boardex['RoleEndYear'] - boardex['RoleStartYear'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cleaned = pd.read_pickle('../data/ceo_funcational_background_titles2_ceo_role.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cleaned = pd.merge(boardex, titles_cleaned, on=['RoleName'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cleaned[['RoleName', 'primary_functional_area']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cleaned['primary_functional_area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = titles_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_functional_background(group):\n",
    "    \"\"\"\n",
    "    Given all the roles for one DirectorID, return exactly one final functional background.\n",
    "    \n",
    "    Rules:\n",
    "      1) If there's any recognized category besides Other/Unclear,\n",
    "         choose the one with the largest total RoleLength.\n",
    "      2) If only Other/Unclear remains => Other/Unclear\n",
    "    \"\"\"\n",
    "    cats = group['primary_functional_area'].unique()\n",
    "    \n",
    "    # 1) First check for any non-Other/Unclear categories\n",
    "    cat_sums = group.groupby('primary_functional_area', dropna=False)['RoleLength'].sum()\n",
    "    # ignore Other/Unclear if there's anything else\n",
    "    cat_sums_meaningful = cat_sums.drop(['Other/Unclear'], errors='ignore')\n",
    "    \n",
    "    if not cat_sums_meaningful.empty:\n",
    "        # largest sum among meaningful categories\n",
    "        top_cat = cat_sums_meaningful.idxmax()\n",
    "        return top_cat\n",
    "    \n",
    "    # 2) If only Other/Unclear is left:\n",
    "    return 'Other/Unclear'\n",
    "\n",
    "def derive_final_backgrounds(df):\n",
    "    # 1) Mark CEO roles; convert Yes or No to 1 and 0\n",
    "    df['is_ceo_role'] = df['is_ceo_role'].map({'Yes': 1, 'No': 0})\n",
    "    # 2) Filter to DirectorIDs who were ever CEO\n",
    "    ceo_ids = (\n",
    "        df.groupby('DirectorID')['is_ceo_role']\n",
    "          .any()\n",
    "          .loc[lambda s: s].index\n",
    "    )\n",
    "    df_ceos = df[df['DirectorID'].isin(ceo_ids)].copy()\n",
    "    \n",
    "    # 3) Group by DirectorID and pick final background\n",
    "    result = (\n",
    "        df_ceos\n",
    "        .groupby('DirectorID')\n",
    "        .apply(pick_functional_background)\n",
    "        .reset_index()                # no 'name=' arg here\n",
    "        .rename(columns={0: 'functional_background'})\n",
    "    )\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_backgrounds = derive_final_backgrounds(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_backgrounds.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_backgrounds['functional_background'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex = boardex.merge(final_backgrounds, on='DirectorID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex['functional_background'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a list of years between RoleStartYear and RoleEndYear.\n",
    "def generate_years(row):\n",
    "    # Check if both start and end years are available.\n",
    "    if pd.notnull(row['RoleStartYear']) and pd.notnull(row['RoleEndYear']):\n",
    "        return list(range(int(row['RoleStartYear']), int(row['RoleEndYear']) + 1))\n",
    "    elif pd.notnull(row['RoleEndYear']):\n",
    "        # Optionally, if RoleStartYear is missing, use just the end year.\n",
    "        return [int(row['RoleEndYear'])]\n",
    "    else:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the 'year' column with the list of years.\n",
    "boardex['year'] = boardex.apply(generate_years, axis=1)\n",
    "\n",
    "# Explode the DataFrame so that each year becomes its own row.\n",
    "boardex_year = boardex.explode('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from thefuzz import fuzz, process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Define a cleaning function for names and companies\n",
    "# ---------------------------------------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercases the string, removes parentheses and their contents,\n",
    "    removes common corporate suffixes, punctuation, and extra whitespace.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return ''\n",
    "    \n",
    "    # Lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Remove anything inside parentheses\n",
    "    s = re.sub(r'\\(.*?\\)', '', s)\n",
    "    \n",
    "    # Remove common legal suffixes (extend this list to fit your needs)\n",
    "    remove_list = [\n",
    "        'inc', 'inc.', 'corp', 'corp.', 'co', 'co.', 'corporation',\n",
    "        'ltd', 'ltd.', 'plc', 'plc.', 'llc', 'limited'\n",
    "    ]\n",
    "    # Use regex to remove each suffix as a standalone word\n",
    "    for word in remove_list:\n",
    "        s = re.sub(r'\\b' + word + r'\\b', '', s)\n",
    "    \n",
    "    # Remove punctuation (except whitespace)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    \n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Create \"clean\" columns in both dataframes\n",
    "# ---------------------------------------------------------\n",
    "boardex_year['clean_company'] = boardex_year['CompanyName'].apply(clean_text)\n",
    "boardex_year['clean_name']    = boardex_year['DirectorName'].apply(clean_text)\n",
    "\n",
    "execucomp['clean_company']    = execucomp['coname'].apply(clean_text)\n",
    "execucomp['clean_name']       = execucomp['exec_fullname'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex_year = boardex_year[boardex_year['year'] >= 2009]\n",
    "execucomp = execucomp[execucomp['year'] >= 2009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardex_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) Extract Unique Company Names from Each Side\n",
    "# ------------------------------------------------------------------------\n",
    "unique_boardex_companies = pd.DataFrame(\n",
    "    boardex_year['clean_company'].unique(),\n",
    "    columns=['clean_company']\n",
    ").dropna()\n",
    "\n",
    "unique_execucomp_companies = pd.DataFrame(\n",
    "    execucomp['clean_company'].unique(),\n",
    "    columns=['clean_company']\n",
    ").dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4) Fuzzy-Match Unique Company Names\n",
    "#    We'll create a function that, for each BoardEx company,\n",
    "#    finds the best match in Execucomp's unique list.\n",
    "# ------------------------------------------------------------------------\n",
    "def fuzzy_match_companies(source_df, target_df, col='clean_company', threshold=80):\n",
    "    \"\"\"\n",
    "    For each 'clean_company' in source_df, find the best fuzzy match \n",
    "    in the 'clean_company' of target_df using thefuzz.process.extractOne.\n",
    "    Returns a DataFrame with columns:\n",
    "      [source_company, matched_company, match_score]\n",
    "    Only returns rows with match_score >= threshold.\n",
    "    \"\"\"\n",
    "    matched_rows = []\n",
    "    target_companies = target_df[col].tolist()\n",
    "\n",
    "    i = 0\n",
    "    total = len(source_df)\n",
    "    \n",
    "    for src_val in source_df[col]:\n",
    "        match_val, score = process.extractOne(\n",
    "            src_val, \n",
    "            target_companies, \n",
    "            scorer=fuzz.token_set_ratio\n",
    "        )\n",
    "\n",
    "        if score >= threshold:\n",
    "            matched_rows.append((src_val, match_val, score))\n",
    "        \n",
    "        print(i, 'of', total, 'done')\n",
    "        i += 1\n",
    "    \n",
    "    out_df = pd.DataFrame(matched_rows, columns=[\n",
    "        f\"{col}_boardex\", f\"{col}_execucomp\", 'company_score'\n",
    "    ])\n",
    "    return out_df\n",
    "\n",
    "company_matches = fuzzy_match_companies(\n",
    "    source_df=unique_execucomp_companies,\n",
    "    target_df=unique_boardex_companies,\n",
    "    col='clean_company',\n",
    "    threshold=80\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5) Merge the Matched Company Table Back to Original Data\n",
    "#    We'll do a left merge on boardex_year, and also left merge on execucomp\n",
    "#    so each row in boardex_year now has the best matched execucomp company,\n",
    "#    and vice versa. \n",
    "#    Then we can join by \"clean_company_execucomp\" or something similar.\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# Merge the company matches to boardex_year\n",
    "boardex_merged = pd.merge(\n",
    "    boardex_year,\n",
    "    company_matches,\n",
    "    left_on='clean_company',\n",
    "    right_on='clean_company_boardex',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge the company matches to execucomp\n",
    "execucomp_merged = pd.merge(\n",
    "    execucomp,\n",
    "    company_matches,\n",
    "    left_on='clean_company',\n",
    "    right_on='clean_company_execucomp',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Now we have:\n",
    "# boardex_merged:\n",
    "#   - clean_company_boardex\n",
    "#   - clean_company_execucomp (the matched one)\n",
    "#   - company_score\n",
    "#\n",
    "# execucomp_merged: (similarly)\n",
    "#   - clean_company_boardex\n",
    "#   - clean_company_execucomp\n",
    "#   - company_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the rows where company_score >= 90\n",
    "boardex_merged = boardex_merged[boardex_merged['company_score'] >= 90]\n",
    "execucomp_merged = execucomp_merged[execucomp_merged['company_score'] >= 90]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6) Next step: Merge boardex_merged and execucomp_merged on:\n",
    "#    1) year (exact)\n",
    "#    2) The matched company name from each side\n",
    "#       i.e. boardex_merged['clean_company_execucomp'] \n",
    "#            vs execucomp_merged['clean_company_execucomp']\n",
    "#\n",
    "#    Because they share the *same* matched Execucomp company name.\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "final_df = pd.merge(\n",
    "    boardex_merged,\n",
    "    execucomp_merged,\n",
    "    on=['year', 'clean_company_execucomp'],  # also consider merging on 'clean_company_boardex' if you prefer\n",
    "    suffixes=('_boardex','_execucomp'),\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# At this point, final_df has columns from both sides, aligned by:\n",
    "# - year\n",
    "# - the fuzzy matched company name in execucomp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 7) If you need to fuzzy match on the *person's name*:\n",
    "#    You can do it now that you have a smaller dataset \n",
    "#    (only the matched companies for the same year).\n",
    "# ------------------------------------------------------------------------\n",
    "final_df['name_score'] = final_df.apply(\n",
    "    lambda x: fuzz.token_set_ratio(x['clean_name_boardex'], x['clean_name_execucomp']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "name_threshold = 80\n",
    "final_df = final_df[final_df['name_score'] >= name_threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select CompanyID\tDirectorID DirectorName\tCompanyName RoleName functional_background\tyear exec_fullname\tgvkey\texecid \n",
    "final_df_select = final_df[[\n",
    "    'CompanyID',\n",
    "    'DirectorID',\n",
    "    'DirectorName',\n",
    "    'CompanyName',\n",
    "    'RoleName',\n",
    "    'functional_background',\n",
    "    'year',\n",
    "    'exec_fullname',\n",
    "    'gvkey',\n",
    "    'execid'\n",
    "]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique execid and functional_background. visualize the distribution\n",
    "final_df_select.drop_duplicates(subset=['execid', 'functional_background']).groupby('functional_background').size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_select.drop_duplicates(subset=['gvkey', 'year', 'execid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = database.merge(final_df_select[['gvkey', 'year', 'execid', 'functional_background']], how='left', on=['gvkey', 'year', 'execid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Add FLS, Guidance, Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLS function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ssl\n",
    "# Bypass SSL certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Ensure you have the required nltk resources\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_forward_looking(sentence, current_year):\n",
    "    # Define the keywords and conjugations for forward-looking criteria\n",
    "    keywords = [\n",
    "        \"will\", \"future\", \"next fiscal\", \"next month\", \"next period\", \"next quarter\", \"next year\",\n",
    "        \"incoming fiscal\", \"incoming month\", \"incoming period\", \"incoming quarter\", \"incoming year\",\n",
    "        \"coming fiscal\", \"coming month\", \"coming period\", \"coming quarter\", \"coming year\",\n",
    "        \"upcoming fiscal\", \"upcoming month\", \"upcoming period\", \"upcoming quarter\", \"upcoming year\",\n",
    "        \"subsequent fiscal\", \"subsequent month\", \"subsequent period\", \"subsequent quarter\", \"subsequent year\",\n",
    "        \"following fiscal\", \"following month\", \"following period\", \"following quarter\", \"following year\"\n",
    "    ]\n",
    "\n",
    "    excluded_keywords = [\"shall\", \"should\", \"can\", \"could\", \"may\", \"might\"]\n",
    "\n",
    "    verbs = [\n",
    "        \"aim\", \"anticipate\", \"assume\", \"commit\", \"estimate\", \"expect\",\n",
    "        \"forecast\", \"foresee\", \"hope\", \"intend\", \"plan\", \"project\",\n",
    "        \"seek\", \"target\"\n",
    "    ]\n",
    "\n",
    "    verb_conjugations = [\n",
    "        \"we \", \"and \", \"but \", \"do not \", \"company \", \"corporation \", \"firm \", \"management \",\n",
    "        \"and \", \"but \", \"does not \", \"is \", \"are \", \"not \", \"is \", \"are \", \"not \",\n",
    "        \"normally \", \"normally \", \"currently \", \"currently \", \"also \", \"also \"\n",
    "    ]\n",
    "\n",
    "    # Search 1: Keyword based search\n",
    "    for keyword in keywords:\n",
    "        if keyword in sentence and not any(excluded in sentence for excluded in excluded_keywords):\n",
    "            return True\n",
    "\n",
    "    # Search 2: Verb conjugation based search\n",
    "    for verb in verbs:\n",
    "        for conj in verb_conjugations:\n",
    "            if f\"{conj}{verb}\" in sentence:\n",
    "                return True\n",
    "\n",
    "    # Search 3: Year reference based search\n",
    "    year_matches = re.findall(r'\\b(20\\d{2})\\b', sentence)\n",
    "    for year in year_matches:\n",
    "        if int(year) > current_year:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def measure_forward_looking_statements(transcript, current_year):\n",
    "    sentences = sent_tokenize(transcript)\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "\n",
    "    forward_looking_sentences = 0\n",
    "    forward_looking_words = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if is_forward_looking(sentence, current_year):\n",
    "            forward_looking_sentences += 1\n",
    "            forward_looking_words += len(word_tokenize(sentence))\n",
    "\n",
    "    forward_looking_sentence_ratio = forward_looking_sentences / total_sentences if total_sentences > 0 else 0\n",
    "    forward_looking_word_ratio = forward_looking_words / total_words if total_words > 0 else 0\n",
    "\n",
    "    return forward_looking_sentence_ratio, forward_looking_word_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "call_transcript = \"\"\"\n",
    "Thank you, Brett. It was a record third quarter powered by the continued strength of Microsoft Cloud, which surpassed $35 billion in revenue, up 23%. Microsoft Copilot and Copilot stack spanning everyday productivity, business process and developer services to models, data and infrastructure are orchestrating a new era of AI transformation driving better business outcomes across every role and industry. Now I'll highlight examples walking up the stack, starting with AI infrastructure.\n",
    "\n",
    "Azure again took share as customers use our platforms and tools to build their own AI solutions. We offer the most diverse selection of AI accelerators, including the latest from NVIDIA, AMD, as well as our own first-party silicon. Our AI innovation continues to build on our strategic partnership with OpenAI, more than 65% of the Fortune 500 now use Azure OpenAI service. We also continue to innovate and partner broadly to bring customers the best selection of frontier models in open-source models, LLMs, and SLMs with 53, which we announced earlier this week, we offer the most capable and cost-effective SLM available.\n",
    "\n",
    "It's already being trialed by companies like CallMiner, LTIMindtree, PwC, and TCS. Our models as a service offering makes it easy for developers to use LLM and SLM without having to manage any underlying infrastructure. Hundreds of paid customers from Accenture and EY to Schneider Electric are using it to take advantage of API access to third-party models, including as of this quarter, the latest from Cohere, Meta, and Mistral. And as part of our partnership announced last week, G42 will run its AI applications and services on our cloud.\n",
    "\n",
    "All up, the number of Azure AI customers continues to grow and average spend continues to increase. We also saw an acceleration of revenue from migrations to Azure. Azure Arc continues to help customers like DICK'S Sporting Goods and World Bank streamlined their cloud migrations. Arc now has 33,000 customers, up over 2x year over year, and we are the hyperscale platform of choice for SAP and Oracle workloads with Conduent and Medline moving their on-premise Oracle Estates to Azure and Kyndryl and L'Oreal migrating their SAP workloads to Azure.\n",
    "\n",
    "Overall, we are seeing an acceleration in the number of large Azure deals from leaders across industries, including billion-dollar-plus, multiyear commitments announced this month from Cloud Software Group and the Coca-Cola Company. The number of $100 million-plus Azure deals increased over 80% year over year, while the number of $10 million-plus deals more than doubled. Now on to data and analytics. Our Microsoft intelligent data platform provides customers with the broadest capability, spanning databases, analytics, business intelligence, governance, and AI.\n",
    "\n",
    "Over half of our Azure AI customers also use our data and analytics tools. Customers are building intelligent applications running on Azure, PostgreSQL, and Cosmos DB with deep integrations with Azure AI. TomTom is a great example. They've used Cosmos DB along with Azure Open AI service to build their own immersive in-car infotainment system.\n",
    "\n",
    "We are also encouraged by our momentum with our next-generation analytics platform, Microsoft Fabric. Fabric now has over 11,000 paid customers, including leaders in every industry from ABB, EDP, Energy Transfer to Equinor, Foot Locker, ITOCHU, and Lumen, and we are seeing increased usage intensity. Fabric is seamlessly integrated with Azure AI studio meaning customers can run models against enterprise data that's consolidated in Fabric's multi-cloud data lake, OneLake. And Power BI, which is also natively integrated with Fabric provides business users with AI-powered insights.\n",
    "\n",
    "We now have over 350,000 paid customers. Now on to developers. GitHub Copilot is bending the productivity curve for developers. We now have 1.8 million paid subscribers with growth accelerating to over 35% quarter over quarter and continues to see increased adoption from businesses in every industry, including Itau, Lufthansa Systems, Nokia, Pinterest, and Volvo cars.\n",
    "\n",
    "Copilot is driving growth across the broader GitHub platform, too. AT&T, Citigroup, and Honeywell all increased their overall getup usage after seeing productivity and code quality increases with Copilot. All up more than 90% of the Fortune 100 are now GitHub customers and revenue accelerated over 45% year over year. Anyone can be a developer with new AI-powered features across our low-code, no-code tools, which makes it easier to build an app, automate workflow or create a Copilot using natural language.\n",
    "\n",
    "Thirty thousand organizations across every industry have used Copilot studio to customize Copilot for Microsoft 365 or build their own, up 175% quarter over quarter. Cineplex, for example, built a Copilot for customer service agents, reducing query handling time from as much as 15 minutes to 30 seconds. All up over 330,000 organizations, including over half of Fortune 100 have used AI-powered capabilities in Power Platform, and Power Apps now has over 25 million monthly active users, up over 40% year over year. Now on to future of work.\n",
    "\n",
    "We are seeing AI democratize expertise across the workforce. What inventory turns are to efficiency of supply chains, knowledge turns, the creation and diffusion, and knowledge are to productivity of an organization and Copilot for Microsoft 365 is helping increase knowledge turns. Thus, having a cascading effect changing work, work artifacts, and workflows, and driving better decision-making, collaboration and efficiency. This quarter, we made Copilot available to organizations of all types and sizes from enterprises to small businesses, nearly 60% of the Fortune 500 now use Copilot and we have seen accelerated adoption across industries and geographies with companies like Amgen, BP, Cognizant, Koch Industries, Moody's, Novo Nordisk, NVIDIA, and Tech Mahindra purchasing over 10,000 seats.\n",
    "\n",
    "We're also seeing increased usage intensity from early adopters, including a nearly 50% increase in the number of Copilot-assisted interactions per user in Teams, bridging group activity with business process workflows and enterprise knowledge. And we're not stopping there. We're accelerating our innovation, adding over 150 Copilot capabilities since the start of the year. With Copilot in Dynamics 365, we are helping businesses transform every role in business function as we take share with our AI-powered apps across all categories.\n",
    "\n",
    "This quarter, we made our Copilot for service and Copilot for sales broadly available, helping customer service agents and sellers at companies like Land O'Lakes, Northern Trust, Rockwell Automation, and Toyota Group generate role-specific insights and recommendations from across Dynamics 365 and Microsoft 365, as well as third-party platforms like Salesforce, ServiceNow, and Zendesk. And with our Copilot for finance, we are drawing context from dynamics, as well as ERP systems like SAP to reduce labor-intensive processes like collections and contract and invoice capture for companies like dentsu and IDC. ISVs are also building their own Copilot integrations. For example, new integrations between Adobe Experience Cloud and Copilot will help marketers access campaign insights in the flow of their work.\n",
    "\n",
    "When it comes to devices, Copilot in Windows is now available on nearly 225 million Windows 10 and Windows 11 PCs, up two times quarter over quarter. With Copilot, we have an opportunity to create an entirely new category of devices, purpose built for this new generation of AI. All of our largest OEM partners have announced AI PCs in recent months. And this quarter, we introduced new surface devices, which include integrated NPUs to power on-device AI experiences like auto framing and live captions.\n",
    "\n",
    "And there's much more to come in just a few weeks, we'll hold a special event to talk about our AI vision across Windows and devices. When it comes to Teams, we once again saw year-over-year usage growth. We're rolling out a new version, which is up to two times faster while using 50% less memory for all customers. We surpassed 1 million Teams rooms for the first time as we continue to make hybrid meetings better with new AI-powered features like automatic camera switching and speaker recognition.\n",
    "\n",
    "And Teams Phone continues to be the market leader in cloud calling now with over 20 million PSTN users, up nearly 30% year over year. All of this innovation is driving growth across Microsoft 365 companies across the private and public sector, including Amadeus, BlackRock, Chevron, Ecolab, Kimberly Clark, all chose our premium E5 offerings this quarter for advanced security, compliance, voice, and analytics. Now on to industry and cross-industry clouds. We are also bringing AI-powered transformation to every industry.\n",
    "\n",
    "In healthcare, DAX Copilot is being used by more than 200 healthcare organizations, including Providence, Stanford Health Care, and WellSpan Health. And in manufacturing, this week, at HANNOVER MESSE, customers like BMW, Siemens, and Volvo Penta, shared how they're using our cloud and AI solutions to transform factory operations. Now on to security. Security underpins every layer of the tech stack and it's our No.\n",
    "\n",
    "1 priority. We launched our Secure Future Initiative last fall for this reason, bringing together every part of the company to advance cybersecurity protection and we are doubling down on this very important work, putting security about all else before all other features and investments. We are focused on making continuous progress across the six pillars of this initiative as we protect tenants and isolate production systems, protect identities and secrets, protect networks, protect engineering systems, monitor and detect threats, and accelerate responses and remediation. We remain committed to sharing our learnings, tools, and innovation with customers.\n",
    "\n",
    "A great example is Copilot for security, which we made generally available earlier this month, bringing together LLM with domain-specific skills informed by our threat intelligence and 78 trillion daily security signals to provide security teams with actionable insights. Now let me talk about our consumer businesses, starting with LinkedIn. We continue to combine our unique data with this new generation of AI to transform the way members learn, sell, and get hired. Features like LinkedIn AI-assisted messages are seeing a 40% higher acceptance rate and accepted over 10% faster by jobseekers saving hires, time and making it easier to connect them to candidates.\n",
    "\n",
    "Our AI-powered collaborative articles, which has reached over 12 million contributions are helping increase engagement on the platform, which reached a new record this quarter. New AI features are also helping accelerate LinkedIn premium growth with revenue up 29% year over year. We are also seeing strength across our other businesses with hiring, taking share for the seventh consecutive quarter. Now on to search advertising and news.\n",
    "\n",
    "We once again took share across Bing and Edge as we continue to apply this new generation of AI to transform how people search and browse. Bing reached over 140 million daily active users, and we are particularly encouraged by our momentum in mobile. Our free Copilot apps on iOS and Android saw a surge in downloads after our Super Bowl ad and are among the highest-rated in this category. We also rolled out Copilot to our ad platform this quarter, helping marketers use AI to generate recommendations for product images, headlines, and descriptions.\n",
    "\n",
    "Now on to gaming. We are committed to meeting players where they are by bringing great games to more people on more devices. We set third quarter records for game streaming hours, console usage, and monthly active devices. And last month, we added our first Activision Blizzard title Diablo 4 to our Game Pass service.\n",
    "\n",
    "Subscribers played over 10 million hours within the first 10 days, making it one of our biggest first-party Game Pass launches ever. We were also encouraged by ongoing success of Call of Duty: Modern Warfare 3, which is attracting new gamers and retaining franchise loyalists. Finally, we are expanding our games to new platforms, bringing four of our fan-favorite titles to Nintendo Switch and Sony PlayStation for the first time. In fact, earlier this month, we had seven games among the top 25 on the PlayStation store more than any other publisher.\n",
    "\n",
    "In closing, I'm energized about our opportunity ahead as we innovate to help people and businesses thrive in this new era. With that, let me turn it over to Amy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "current_year = 2024\n",
    "forward_looking_sentence_ratio, forward_looking_word_ratio = measure_forward_looking_statements(call_transcript, current_year)\n",
    "print(f\"Forward-Looking Sentence Ratio: {forward_looking_sentence_ratio:.4f}\")\n",
    "print(f\"Forward-Looking Word Ratio: {forward_looking_word_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_transcripts =pd.read_pickle(\"../data/sxp1500_presentations_ceo_aggregated.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_transcripts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function on the first transcript\n",
    "forward_looking_sentence_ratio, forward_looking_word_ratio = measure_forward_looking_statements(aggregated_transcripts['transcript_text'][0], aggregated_transcripts['year'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there duplicates for transcriptid?\n",
    "aggregated_transcripts['transcriptid'].nunique() == aggregated_transcripts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to all transcripts, save in two new columns the forward looking sentence ratio and the forward looking word ratio\n",
    "aggregated_transcripts[['forward_looking_sentence_ratio', 'forward_looking_word_ratio']] = aggregated_transcripts.apply(lambda x: pd.Series(measure_forward_looking_statements(x['transcript_text'], x['year'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary statistics of the forward looking sentence ratio and the forward looking word ratio\n",
    "print(aggregated_transcripts[['forward_looking_sentence_ratio', 'forward_looking_word_ratio']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe with the new columns; only transcriptid and the two new columns\n",
    "aggregated_transcripts[['transcriptid', 'forward_looking_sentence_ratio', 'forward_looking_word_ratio']].to_pickle(\"../data/sxp1500_presentations_ceo_aggregated_forward_looking_v12.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregated_transcripts = pd.read_pickle(\"../data/sxp1500_presentations_ceo_aggregated_forward_looking_v11.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance = pd.read_stata(\"../data/guidance_data_bill_20240616.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadate column to datetime\n",
    "guidance['datadate'] = pd.to_datetime(guidance['datadate'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadate column statistics\n",
    "guidance['datadate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GVKEY column rename to gvkey\n",
    "guidance.rename(columns={'GVKEY': 'gvkey'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdq column to datetime\n",
    "guidance['rdq'] = pd.to_datetime(guidance['rdq'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with regressions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the two variables merging on transcriptid\n",
    "database2 = database.merge(aggregated_transcripts[['transcriptid', 'forward_looking_sentence_ratio', 'forward_looking_word_ratio']], on='transcriptid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(database2[['forward_looking_sentence_ratio', 'forward_looking_word_ratio']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadate format to datetime\n",
    "# print a sample first\n",
    "print(database2['datadate'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "database2['datadate'] = pd.to_datetime(database2['datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe datadate\n",
    "print(database2['datadate'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostimportantdateutc format to datetime\n",
    "database2['mostimportantdateutc'] = pd.to_datetime(database2['mostimportantdateutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the guidance data with the database based on gvkey and datadate\n",
    "database2 = database2.merge(guidance, on=['gvkey', 'datadate'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "database2.to_csv(\"../data/sxp1500_presentations_ceo_aggregated_regression_vars_cleaned_fls.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mcdonald dictionary\n",
    "mcd_dic = pd.read_csv('../data/Loughran-McDonald_MasterDictionary_1993-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcd_dic.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `df` is your dataframe containing the earnings calls transcripts in the column 'transcript_text'\n",
    "# Assuming `mcd_dic` is your dataframe containing the words categorized by 'Negative', 'Positive', etc.\n",
    "\n",
    "# Lists of words converted to lowercase\n",
    "negative_list = set(mcd_dic[mcd_dic['Negative'] > 0]['Word'].str.lower().tolist())\n",
    "positive_list = set(mcd_dic[mcd_dic['Positive'] > 0]['Word'].str.lower().tolist())\n",
    "uncertainty_list = set(mcd_dic[mcd_dic['Uncertainty'] > 0]['Word'].str.lower().tolist())\n",
    "litigious_list = set(mcd_dic[mcd_dic['Litigious'] > 0]['Word'].str.lower().tolist())\n",
    "strong_modal_list = set(mcd_dic[mcd_dic['Strong_Modal'] > 0]['Word'].str.lower().tolist())\n",
    "weak_modal_list = set(mcd_dic[mcd_dic['Weak_Modal'] > 0]['Word'].str.lower().tolist())\n",
    "complexity_list = set(mcd_dic[mcd_dic['Complexity'] > 0]['Word'].str.lower().tolist())\n",
    "\n",
    "# Function to count occurrences of words from a list in a text using Counter\n",
    "def count_words(text, word_set):\n",
    "    words = re.findall(r'\\w+', text.lower())  # Convert text to lowercase and find all words\n",
    "    counter = Counter(words)\n",
    "    count = sum(counter[word] for word in word_set)\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transcript_text column to lowercase\n",
    "aggregated_transcripts['transcript_text'] = aggregated_transcripts['transcript_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each transcript and create new columns\n",
    "aggregated_transcripts['negative_count'] = aggregated_transcripts['transcript_text'].apply(lambda x: count_words(x, negative_list))\n",
    "print('negative count done')\n",
    "\n",
    "aggregated_transcripts['positive_count'] = aggregated_transcripts['transcript_text'].apply(lambda x: count_words(x, positive_list))\n",
    "print('positive count done')\n",
    "\n",
    "aggregated_transcripts['uncertainty_count'] = aggregated_transcripts['transcript_text'].apply(lambda x: count_words(x, uncertainty_list))\n",
    "print('uncertainty count done')\n",
    "\n",
    "aggregated_transcripts['complexity_count'] = aggregated_transcripts['transcript_text'].apply(lambda x: count_words(x, complexity_list))\n",
    "\n",
    "# Calculate total word count for each transcript\n",
    "aggregated_transcripts['total_word_count'] = aggregated_transcripts['transcript_text'].apply(lambda x: len(re.findall(r'\\w+', x.lower())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ratio of each sentiment word count to the total word count\n",
    "aggregated_transcripts['negative_ratio'] = aggregated_transcripts['negative_count'] / aggregated_transcripts['total_word_count']\n",
    "aggregated_transcripts['positive_ratio'] = aggregated_transcripts['positive_count'] / aggregated_transcripts['total_word_count']\n",
    "aggregated_transcripts['uncertainty_ratio'] = aggregated_transcripts['uncertainty_count'] / aggregated_transcripts['total_word_count']\n",
    "aggregated_transcripts['complexity_ratio'] = aggregated_transcripts['complexity_count'] / aggregated_transcripts['total_word_count']\n",
    "\n",
    "# add positve - negative / total * 100\n",
    "aggregated_transcripts['sentiment_score'] = (aggregated_transcripts['positive_count'] - aggregated_transcripts['negative_count']) / aggregated_transcripts['total_word_count'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print descriptive statistics of the sentiment ratios\n",
    "print(aggregated_transcripts[['negative_ratio', 'positive_ratio', 'uncertainty_ratio', 'complexity_ratio', 'sentiment_score']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the new columns, merging on transcriptid\n",
    "database2 = database2.merge(aggregated_transcripts[['transcriptid', 'negative_ratio', 'positive_ratio', 'uncertainty_ratio',  'complexity_ratio', 'sentiment_score']], on='transcriptid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database2.to_csv(\"../data/sxp1500_presentations_ceo_aggregated_regression_vars_cleaned_fls_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - Uncertainty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open firmquarter_2022q1.csv\n",
    "uncertainty = pd.read_stata('../data/firmquarter_2022q1.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty = uncertainty[['gvkey', 'PRisk', 'NPRisk', 'date_earningscall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date_earningscall to datetime\n",
    "# it is currently like 27-Jun-2002\t\n",
    "uncertainty['date_earningscall_dt'] = pd.to_datetime(uncertainty['date_earningscall'], format='%d-%b-%Y')\n",
    "uncertainty['gvkey'] = uncertainty['gvkey'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty.date_earningscall_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database2.mostimportantdateutc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database2['mostimportantdateutc'] = pd.to_datetime(database2['mostimportantdateutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, sort both DataFrames by their date columns\n",
    "database2 = database2.sort_values('mostimportantdateutc')\n",
    "uncertainty = uncertainty.sort_values('date_earningscall_dt')\n",
    "\n",
    "# Perform an asof merge with a 30-day tolerance\n",
    "database3 = pd.merge_asof(\n",
    "    database2,\n",
    "    uncertainty,\n",
    "    left_on='mostimportantdateutc',\n",
    "    right_on='date_earningscall_dt',\n",
    "    by='gvkey',\n",
    "    tolerance=pd.Timedelta(days=30),\n",
    "    direction='nearest'  # Options: 'backward', 'forward', or 'nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Policy Uncertainty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US economic data\n",
    "econ_uncertainty = pd.read_excel('../data/US_Policy_Uncertainty_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_uncertainty.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_uncertainty.columns = econ_uncertainty.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the last row of data in econ_uncertainty\n",
    "econ_uncertainty = econ_uncertainty.iloc[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make year and month int in both dataframes; drop if an issue \n",
    "econ_uncertainty['year'] = econ_uncertainty['year'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create trailing average of 12 month of US economic uncertainty\n",
    "econ_uncertainty['trailing_12month_uncertainty_3component'] = econ_uncertainty['three_component_index'].transform(lambda x: x.rolling(window=12).mean())\n",
    "econ_uncertainty['trailing_12month_uncertainty_newsbased'] = econ_uncertainty['news_based_policy_uncert_index'].transform(lambda x: x.rolling(window=12).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_uncertainty.columns = ['year_1month_lag', 'month_1month_lag', 'three_component_index',\n",
    "       'news_based_policy_uncert_index',\n",
    "       'trailing_12month_uncertainty_3component',\n",
    "       'trailing_12month_uncertainty_newsbased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with same year but previous month in database3\n",
    "database3['date_1month_lag'] = database3['mostimportantdateutc'] - pd.DateOffset(months=0)\n",
    "database3['year_1month_lag'] = database3['date_1month_lag'].dt.year\n",
    "database3['month_1month_lag'] = database3['date_1month_lag'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database4 = pd.merge(database3, econ_uncertainty, on=['year_1month_lag', 'month_1month_lag'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - Slack vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# STEP 1: DOWNLOAD QUARTERLY DATA (no SICH) & ANNUAL DATA (with SICH)\n",
    "###############################################################################\n",
    "\n",
    "previously_downloaded_compustat_quarterly_slack = True\n",
    "if not previously_downloaded_compustat_quarterly_slack:\n",
    "    db = wrds.Connection()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1A. Query QUARTERLY data (fundq) - does NOT have SICH, but we need at, sale, che, emp, etc.\n",
    "    #     We'll alias fyearq AS fyear for easy merging later.\n",
    "    # -------------------------------------------------------------------------\n",
    "    query_q = \"\"\"\n",
    "        SELECT\n",
    "            gvkey,\n",
    "            datadate,\n",
    "            fyearq AS fyear,        -- rename so we can merge on 'fyear'\n",
    "            fqtr,                   -- fiscal quarter if needed\n",
    "            cusip,\n",
    "            atq  AS at,             -- total assets (quarterly)\n",
    "            saleq AS sale,          -- net sales (quarterly)\n",
    "            cheq AS che,            -- cash & short-term investments (quarterly)\n",
    "            cogsq AS cogs,          -- cost of goods sold\n",
    "            revtq AS revt           -- total revenue\n",
    "        FROM comp_na_daily_all.fundq\n",
    "        WHERE fyearq >= 2002\n",
    "          AND indfmt = 'INDL'\n",
    "          AND datafmt = 'STD'\n",
    "          AND popsrc = 'D'\n",
    "          AND consol = 'C'\n",
    "    \"\"\"\n",
    "    compustat_q_slack = db.raw_sql(query_q)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1B. Query ANNUAL data (funda) - we only need gvkey, fyear, and sich for the merge\n",
    "    # -------------------------------------------------------------------------\n",
    "    query_a_sich = \"\"\"\n",
    "        SELECT\n",
    "            gvkey,\n",
    "            fyear,\n",
    "            sich,\n",
    "            emp\n",
    "        FROM comp_na_daily_all.funda\n",
    "        WHERE fyear >= 2002\n",
    "          AND indfmt = 'INDL'\n",
    "          AND datafmt = 'STD'\n",
    "          AND popsrc = 'D'\n",
    "          AND consol = 'C'\n",
    "    \"\"\"\n",
    "    compustat_a_sich = db.raw_sql(query_a_sich)\n",
    "    \n",
    "    # Save both\n",
    "    compustat_q_slack.to_pickle('compustat_q_slack.pkl')\n",
    "    compustat_a_sich.to_pickle('compustat_a_sich.pkl')\n",
    "    \n",
    "    db.close()\n",
    "else:\n",
    "    compustat_q_slack = pd.read_pickle('compustat_q_slack.pkl')\n",
    "    compustat_a_sich  = pd.read_pickle('compustat_a_sich.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# STEP 2: MERGE ANNUAL SICH INFO ONTO QUARTERLY DATA\n",
    "#         We will match on (gvkey, fyear).\n",
    "###############################################################################\n",
    "\n",
    "# Just keep the columns needed for merging: gvkey, fyear, sich\n",
    "compustat_a_sich = compustat_a_sich[['gvkey','fyear','sich', 'emp']].drop_duplicates()\n",
    "\n",
    "# Merge left: keep all quarterly obs, bring in 'sich' from annual\n",
    "compustat_q_slack = pd.merge(\n",
    "    compustat_q_slack,\n",
    "    compustat_a_sich,\n",
    "    how='left',\n",
    "    on=['gvkey','fyear']    # merges annual fyear with quarterly fyear\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# STEP 3: BASIC CLEANING\n",
    "###############################################################################\n",
    "\n",
    "# Convert numeric columns if needed\n",
    "numeric_cols = ['at','sale','che','emp','cogs','revt']\n",
    "for col in numeric_cols:\n",
    "    compustat_q_slack[col] = pd.to_numeric(compustat_q_slack[col], errors='coerce')\n",
    "\n",
    "# Drop rows with missing or zero in key fields used for slack\n",
    "compustat_q_slack.dropna(subset=['at','sale','che','emp'], inplace=True)\n",
    "compustat_q_slack = compustat_q_slack[compustat_q_slack['at'] > 0]\n",
    "compustat_q_slack = compustat_q_slack[compustat_q_slack['sale'] > 0]\n",
    "\n",
    "# If you want a 'year' column (already have fyear from the quarterly dataset),\n",
    "# just rename or confirm it:\n",
    "compustat_q_slack['year'] = compustat_q_slack['fyear']\n",
    "\n",
    "###############################################################################\n",
    "# STEP 4: CREATE PERFORMANCE MEASURE\n",
    "#         E.g., Vanacker et al. (2017) measure: (revt - cogs) / at\n",
    "###############################################################################\n",
    "\n",
    "compustat_q_slack['gross_profit'] = compustat_q_slack['revt'] - compustat_q_slack['cogs']\n",
    "compustat_q_slack['perf_gpa'] = compustat_q_slack['gross_profit'] / compustat_q_slack['at']\n",
    "\n",
    "###############################################################################\n",
    "# STEP 5: CONSTRUCT FINANCIAL SLACK & HR SLACK\n",
    "#         - FIN_SLACK = (che / at) minus industry-year mean\n",
    "#         - HR_SLACK  = (emp / sale) minus industry-year mean\n",
    "###############################################################################\n",
    "\n",
    "# (a) raw ratios\n",
    "compustat_q_slack['fin_slack_raw'] = compustat_q_slack['che'] / compustat_q_slack['at']\n",
    "\n",
    "# (b) ensure sich is string for grouping\n",
    "compustat_q_slack['sich'] = compustat_q_slack['sich'].fillna('0000').astype(str)\n",
    "\n",
    "# (c) group by year & sich\n",
    "group_cols = ['year','sich']\n",
    "\n",
    "compustat_q_slack['fin_slack_mean'] = compustat_q_slack.groupby(group_cols)['fin_slack_raw'].transform('mean')\n",
    "\n",
    "# (d) adjusted slack = raw - group mean\n",
    "compustat_q_slack['fin_slack_ind_adjusted'] = compustat_q_slack['fin_slack_raw'] - compustat_q_slack['fin_slack_mean']\n",
    "\n",
    "###############################################################################\n",
    "# STEP 6: (OPTIONAL) WINSORIZE EXTREME VALUES\n",
    "###############################################################################\n",
    "\n",
    "for col in ['fin_slack_ind_adjusted','fin_slack_raw','perf_gpa']:\n",
    "    lower_q = compustat_q_slack[col].quantile(0.005)\n",
    "    upper_q = compustat_q_slack[col].quantile(0.995)\n",
    "    compustat_q_slack[col] = np.clip(compustat_q_slack[col], lower_q, upper_q)\n",
    "\n",
    "###############################################################################\n",
    "# STEP 7: DONE! EXAMPLE PREVIEW\n",
    "###############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat_q_slack = compustat_q_slack[['gvkey', 'datadate', 'fin_slack_ind_adjusted','fin_slack_raw','perf_gpa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat_q_slack['datadate'] = pd.to_datetime(compustat_q_slack['datadate'])\n",
    "compustat_q_slack['gvkey'] = compustat_q_slack['gvkey'].astype(int)\n",
    "compustat_q_slack.drop_duplicates(subset=['gvkey', 'datadate'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database5 = pd.merge(database4, compustat_q_slack, on=['gvkey', 'datadate'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 - Cleaning up Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_db = database5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace database['ceo_years_left']  with nan if 0\n",
    "final_db['ceo_years_left'] = final_db['ceo_years_left'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winsorize earnings_surprise var in 1 percentile\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "final_db['earnings_surprise_wins'] = winsorize(final_db['earnings_surprise'], limits=[0.025, 0.025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or infinite values in 'promises_count' or 'ratio_roa'\n",
    "\n",
    "final_db['ceo_options_compensation_log'] = np.log(final_db['ceo_options_compensation']+1)\n",
    "final_db['ceo_dual_dummy'] = np.where(final_db['ceo_dual']==1, 1, 0) \n",
    "final_db['numest'].fillna(0, inplace=True)\n",
    "final_db['word_count_total_log'] = np.log(final_db['word_count_total']+1)\n",
    "final_db['ceo_last_3_years'] = np.where(final_db['ceo_years_left']<=3, 1, 0)\n",
    "final_db['ceo_last_2_years'] = np.where(final_db['ceo_years_left']<=2, 1, 0)\n",
    "final_db['ceo_last_1_years'] = np.where(final_db['ceo_years_left']<=1, 1, 0)\n",
    "\n",
    "final_db['at_log'] = np.log(final_db['atq']+1)\n",
    "final_db.sort_values(by=['gvkey','mostimportantdateutc'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "final_db['constant'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to stata\n",
    "# first make sure that the variables are in the right format, no infitie values, etc.\n",
    "final_db.replace([np.inf, -np.inf], np.nan).to_csv('../data/sxp1500_presentations_ceo_aggregated_regression_vars_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_db.drop_duplicates(subset=['execid'])['ceo_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding abnormal returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('../data/sxp1500_presentations_ceo_aggregated_regression_vars_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file with cusip and date\n",
    "with open('../data/cusip_dates.txt', 'w') as f:\n",
    "    for _, row in db[['cusip', 'mostimportantdateutc']].iterrows():\n",
    "        f.write(f\"{row['cusip']} {row['mostimportantdateutc']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load abnormal returns data\n",
    "abnormal_returns_7_edate = pd.read_csv('../data/abnormal_returns_7_edate.csv')\n",
    "abnormal_returns_3_edate = pd.read_csv('../data/abnormal_returns_3_edate.csv')\n",
    "abnormal_returns_7 = pd.read_csv('../data/abnormal_returns_7.csv')\n",
    "abnormal_returns_3 = pd.read_csv('../data/abnormal_returns_3.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_returns_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_returns_3_edate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cusip to lowercase in abnormal returns dataframes\n",
    "abnormal_returns_3_edate['cusip'] = abnormal_returns_3_edate['cusip'].str.lower()\n",
    "abnormal_returns_7_edate['cusip'] = abnormal_returns_7_edate['cusip'].str.lower()\n",
    "abnormal_returns_3['cusip'] = abnormal_returns_3['cusip'].str.lower()\n",
    "abnormal_returns_7['cusip'] = abnormal_returns_7['cusip'].str.lower()\n",
    "\n",
    "# Convert cusip to lowercase in the database\n",
    "db['cusip'] = db['cusip'].str.lower()\n",
    "\n",
    "# Convert date columns to datetime for proper merging\n",
    "abnormal_returns_3_edate['evtdate'] = pd.to_datetime(abnormal_returns_3_edate['evtdate'])\n",
    "abnormal_returns_7_edate['evtdate'] = pd.to_datetime(abnormal_returns_7_edate['evtdate'])\n",
    "abnormal_returns_3['evtdate'] = pd.to_datetime(abnormal_returns_3['evtdate'])\n",
    "abnormal_returns_7['evtdate'] = pd.to_datetime(abnormal_returns_7['evtdate'])\n",
    "db['mostimportantdateutc'] = pd.to_datetime(db['mostimportantdateutc'])\n",
    "\n",
    "# Merge all abnormal returns with the database\n",
    "db_with_returns_3_edate = db.merge(abnormal_returns_3_edate, \n",
    "                             left_on=['cusip', 'mostimportantdateutc'],\n",
    "                             right_on=['cusip', 'evtdate'],\n",
    "                             how='left')\n",
    "\n",
    "db_with_returns_7_edate = db.merge(abnormal_returns_7_edate,\n",
    "                             left_on=['cusip', 'mostimportantdateutc'],\n",
    "                             right_on=['cusip', 'evtdate'],\n",
    "                             how='left')\n",
    "\n",
    "db_with_returns_3 = db.merge(abnormal_returns_3,\n",
    "                             left_on=['cusip', 'mostimportantdateutc'],\n",
    "                             right_on=['cusip', 'evtdate'],\n",
    "                             how='left')\n",
    "\n",
    "db_with_returns_7 = db.merge(abnormal_returns_7,\n",
    "                             left_on=['cusip', 'mostimportantdateutc'],\n",
    "                             right_on=['cusip', 'evtdate'],\n",
    "                             how='left')\n",
    "\n",
    "# Merge all into one database\n",
    "db_merged = db.copy()\n",
    "\n",
    "# Add 3-day event date returns\n",
    "db_merged = db_merged.merge(abnormal_returns_3_edate[['cusip', 'evtdate', 'cret', 'car', 'bhar']], \n",
    "                           left_on=['cusip', 'mostimportantdateutc'],\n",
    "                           right_on=['cusip', 'evtdate'],\n",
    "                           how='left',\n",
    "                           suffixes=('', '_3_edate'))\n",
    "\n",
    "# Add 7-day event date returns\n",
    "db_merged = db_merged.merge(abnormal_returns_7_edate[['cusip', 'evtdate', 'cret', 'car', 'bhar']], \n",
    "                           left_on=['cusip', 'mostimportantdateutc'],\n",
    "                           right_on=['cusip', 'evtdate'],\n",
    "                           how='left',\n",
    "                           suffixes=('', '_7_edate'))\n",
    "\n",
    "# Add 3-day returns\n",
    "db_merged = db_merged.merge(abnormal_returns_3[['cusip', 'evtdate', 'ret', 'abret']], \n",
    "                           left_on=['cusip', 'mostimportantdateutc'],\n",
    "                           right_on=['cusip', 'evtdate'],\n",
    "                           how='left',\n",
    "                           suffixes=('', '_3'))\n",
    "\n",
    "# Add 7-day returns\n",
    "db_merged = db_merged.merge(abnormal_returns_7[['cusip', 'evtdate', 'ret', 'abret']], \n",
    "                           left_on=['cusip', 'mostimportantdateutc'],\n",
    "                           right_on=['cusip', 'evtdate'],\n",
    "                           how='left',\n",
    "                           suffixes=('', '_7'))\n",
    "\n",
    "# Clean up duplicate columns\n",
    "db_merged.drop(columns=['evtdate', 'evtdate_3_edate', 'evtdate_7_edate', 'evtdate_3', 'evtdate_7'], \n",
    "               errors='ignore', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding CEO broken promises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('../data/sxp1500_presentations_ceo_aggregated_regression_vars_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Load and prepare data (unchanged)\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "promises = pd.read_csv(\"../data/sxp1500_presentations_ceo_aggregated_promises_expanded_cleaned_transcriptlevel_horizon_specificity.csv\")\n",
    "# create promise_id column, it is gvkey_transcriptid_2digitnumber (01, 02, 03, ...)\n",
    "promises['promise_id'] = promises.groupby(['gvkey', 'transcriptid']).cumcount() + 1\n",
    "promises['promise_id'] = promises['gvkey'].astype(str) + '_' + promises['transcriptid'].astype(str) + '_' + promises['promise_id'].apply(lambda x: f'{x:02d}')\n",
    "promises_select = promises[['gvkey', 'mostimportantdateutc', 'transcriptid', 'companyname', 'exec_fullname', 'execid', 'promise_id','1-promise-verbatim' ,'2-promise-explain' ,'3-promise-horizon-v2', 'specificity_score']].sort_values(by=['gvkey', 'mostimportantdateutc',])\n",
    "\n",
    "labels = pd.read_csv(\"promises_with_keywords_v5_labels.csv\")\n",
    "def revert_promise_id(promise_id):\n",
    "    parts = promise_id.split('_')\n",
    "    fixed_parts = []\n",
    "    for part in parts:\n",
    "        if part.endswith('.0'):\n",
    "            part = str(int(float(part)))\n",
    "        fixed_parts.append(part)\n",
    "    return '_'.join(fixed_parts)\n",
    "\n",
    "labels['promise_id'] = labels['promise_id'].apply(revert_promise_id)\n",
    "\n",
    "labels = labels[['promise_id', 'primary_keyword']]\n",
    "\n",
    "# merge promises and labels on promise_id\n",
    "promises_select = pd.merge(labels, promises_select, on=['promise_id'], how='left')\n",
    "\n",
    "# %%\n",
    "promises_select.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promises_select_10percent_results = pd.read_csv(\"promises_select_10percent_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promises_select_10percent_results_batch2 = pd.read_csv(\"promises_select_10percent_batch2_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promises_select_10percent_results = pd.concat([promises_select_10percent_results, promises_select_10percent_results_batch2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with promises_select_10percent on promise_id\n",
    "promises_select_10percent_results_merged = pd.merge(promises_select_10percent_results, promises_select, on=['promise_id'], how='left')\n",
    "\n",
    "# %%\n",
    "\n",
    "promises_select_10percent_results_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promises_select_10percent_results_merged['mostimportantdateutc'] = pd.to_datetime(\n",
    "    promises_select_10percent_results_merged['mostimportantdateutc'], errors='coerce'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promises_select_10percent_results_merged['mostimportantdate_year'] = promises_select_10percent_results_merged['mostimportantdateutc'].dt.year\n",
    "promises_select_10percent_results_merged['mostimportantdate_quarter'] = promises_select_10percent_results_merged['mostimportantdateutc'].dt.quarter\n",
    "promises_select_10percent_results_merged['mostimportantdate_month'] = promises_select_10percent_results_merged['mostimportantdateutc'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 5 rows of the dataframe the date columns\n",
    "promises_select_10percent_results_merged[['mostimportantdateutc', 'mostimportantdate_year', 'mostimportantdate_quarter', 'mostimportantdate_month']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = promises_select_10percent_results_merged.copy()\n",
    "\n",
    "df['mostimportantdate'] = pd.to_datetime(df['mostimportantdateutc'], errors='coerce')\n",
    "df['quarter'] = df['mostimportantdate'].dt.to_period('Q')\n",
    "df['is_broken'] = df['status_code'].isin(['DELAYED', 'NOT_DELIVERED']).astype(int)\n",
    "\n",
    "# per exec-quarter: broken count and total promises\n",
    "by_qtr = (df.groupby(['execid', 'quarter'])\n",
    "            .agg(broken_count=('is_broken', 'sum'),\n",
    "                 promises_in_quarter=('execid', 'size'))\n",
    "            .reset_index())\n",
    "\n",
    "# cumulative *prior* quarters\n",
    "by_qtr = by_qtr.sort_values(['execid', 'quarter'])\n",
    "by_qtr['no_broken_promises'] = (\n",
    "    by_qtr.groupby('execid')['broken_count'].cumsum().shift(1, fill_value=0).astype(int)\n",
    ")\n",
    "by_qtr['no_promises_prior'] = (\n",
    "    by_qtr.groupby('execid')['promises_in_quarter'].cumsum().shift(1, fill_value=0).astype(int)\n",
    ")\n",
    "\n",
    "# --- Rolling windows over prior calendar quarters (8, 12, 20) ---\n",
    "roll_list = []\n",
    "for eid, g in by_qtr.groupby('execid', sort=False):\n",
    "    q_min, q_max = g['quarter'].min(), g['quarter'].max()\n",
    "    idx = pd.period_range(q_min, q_max, freq='Q', name='quarter')\n",
    "\n",
    "    tmp = (g.set_index('quarter')[['broken_count', 'promises_in_quarter']]\n",
    "             .reindex(idx, fill_value=0))\n",
    "\n",
    "    for w in (8, 12, 20):\n",
    "        tmp[f'no_broken_promises_roll{w}'] = (\n",
    "            tmp['broken_count'].rolling(window=w, min_periods=1).sum()\n",
    "            .shift(1, fill_value=0)\n",
    "        ).astype(int)\n",
    "        tmp[f'no_promises_prior_roll{w}'] = (\n",
    "            tmp['promises_in_quarter'].rolling(window=w, min_periods=1).sum()\n",
    "            .shift(1, fill_value=0)\n",
    "        ).astype(int)\n",
    "\n",
    "    tmp['execid'] = eid\n",
    "    roll_list.append(tmp.reset_index()[['execid', 'quarter', \n",
    "        'no_broken_promises_roll8', 'no_broken_promises_roll12', 'no_broken_promises_roll20',\n",
    "        'no_promises_prior_roll8', 'no_promises_prior_roll12', 'no_promises_prior_roll20']])\n",
    "\n",
    "roll_df = pd.concat(roll_list, ignore_index=True)\n",
    "\n",
    "# merge rolling features back to the compact per-quarter table\n",
    "result = (by_qtr.merge(roll_df, on=['execid', 'quarter'], how='left'))\n",
    "\n",
    "# Now merge back transcriptid and mostimportantdateutc from the original df\n",
    "# For each execid/quarter, get the unique transcriptid and mostimportantdateutc values\n",
    "meta = (df[['execid', 'quarter', 'transcriptid', 'mostimportantdateutc']]\n",
    "           .drop_duplicates(subset=['execid', 'quarter', 'transcriptid', 'mostimportantdateutc']))\n",
    "\n",
    "# If there are multiple transcriptids per execid/quarter, this will keep all combinations.\n",
    "# If you want just one transcriptid per execid/quarter, you can do .drop_duplicates(['execid', 'quarter'])\n",
    "# Here, let's keep all combinations for completeness.\n",
    "\n",
    "result = result.merge(meta, on=['execid', 'quarter'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reorder columns to put transcriptid and mostimportantdateutc up front if you want\n",
    "result = result[['transcriptid', \n",
    "                 'no_broken_promises', 'no_promises_prior', 'promises_in_quarter',\n",
    "                 'no_broken_promises_roll8', 'no_broken_promises_roll12', 'no_broken_promises_roll20',\n",
    "                 'no_promises_prior_roll8', 'no_promises_prior_roll12', 'no_promises_prior_roll20']]\n",
    "\n",
    "# result is your final table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with db on transcriptid\n",
    "db_merged = pd.merge(db, result, on=['transcriptid'], how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_merged.to_csv('../data/sxp1500_presentations_ceo_aggregated_regression_vars_cleaned_with_promises_brokenpromises.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing product lunches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('../data/sxp1500_presentations_ceo_aggregated_regression_vars_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = pd.read_pickle(\"../data/sxp1500_presentations_ceo_aggregated.pkl\")\n",
    "transcripts.gvkey.nunique()\n",
    "transcripts.columns\n",
    "# check to see if there are duplicate transcriptid values per keydevid\n",
    "transcripts.groupby(['keydevid'])['transcriptid'].apply(lambda x: x.nunique()>1).sum()\n",
    "# number of words in transcript_text\n",
    "transcripts['word_count_total'] = transcripts['transcript_text'].str.split().str.len()\n",
    "transcripts = transcripts.drop_duplicates(subset=['keydevid'], keep='first')\n",
    "database = transcripts[['companyid', 'keydevid', 'transcriptid', 'mostimportantdateutc', 'mostimportanttimeutc', 'gvkey', 'companyname', 'word_count_total']]\n",
    "database['mostimportantdateutc_dt'] = pd.to_datetime(database['mostimportantdateutc'], format='%Y-%m-%d')\n",
    "#database['year'] = database['mostimportantdateutc_dt'].dt.year\n",
    "#database['month'] = database['mostimportantdateutc_dt'].dt.month\n",
    "# 2 - Promises identified\n",
    "promises = pd.read_csv(\"../data/sxp1500_presentations_ceo_aggregated_promises_expanded_cleaned_transcriptlevel_horizon_specificity.csv\")\n",
    "# create promise_id column, it is gvkey_transcriptid_2digitnumber (01, 02, 03, ...)\n",
    "\n",
    "promises['promise_id'] = promises.groupby(['gvkey', 'transcriptid']).cumcount() + 1\n",
    "promises['promise_id'] = promises['gvkey'].astype(str) + '_' + promises['transcriptid'].astype(str) + '_' + promises['promise_id'].apply(lambda x: f'{x:02d}')\n",
    "\n",
    "\n",
    "list(promises.columns)\n",
    "promises['3-promise-horizon-v2'].value_counts()\n",
    "### Cleaning up the horizons column\n",
    "# if it contains 'unclear' or \"Unclear\" in the promise, then set the promise horizon to 'unclear'\n",
    "promises.loc[promises['3-promise-horizon-v2'].str.contains('unclear', case=False, na=False), '3-promise-horizon-v2'] = 'unclear'\n",
    "\n",
    "def process_value(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    if value == 'unclear':\n",
    "        return np.nan\n",
    "    if not str(value).replace('-', '').replace('.', '').isdigit():\n",
    "        return np.nan\n",
    "    if '-' in value:\n",
    "        try:\n",
    "            number1, number2 = value.split('-')\n",
    "            return (float(number1) + float(number2)) / 2\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "promises['promise_horizon_months'] = promises['3-promise-horizon-v2'].apply(process_value)\n",
    "\n",
    "promises['promise_horizon_months'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promises['promise_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"promises_with_keywords_v5_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['promise_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[['promise_id', 'primary_keyword']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge promises and labels on promise_id\n",
    "promises = pd.merge(labels, promises, on=['promise_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promises['primary_keyword'].value_counts()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Promises count\n",
    "def process_promises(filtered_promises, suffix):\n",
    "    # promises count\n",
    "    filtered_promises[f'promises{suffix}_count_nolaunch'] = filtered_promises.groupby(['transcriptid'])['promise_id'].transform('nunique')\n",
    "\n",
    "    # promises deliver date averaging\n",
    "    filtered_promises[f'promises{suffix}_horizon_nolaunch'] = filtered_promises.groupby(['transcriptid'])[f'promise_horizon_months'].transform(lambda x: x.mean(skipna=True))\n",
    "\n",
    "    # proportion of horizons that are nan per transcript id\n",
    "    filtered_promises[f'promises{suffix}_horizon_nan_nolaunch'] = filtered_promises.groupby(['transcriptid'])['promise_horizon_months'].transform(lambda x: x.isna().sum()/len(x))\n",
    "    \n",
    "    filtered_promises[f'promises{suffix}_specificity_score_nolaunch'] = filtered_promises.groupby(['transcriptid'])['specificity_score'].transform(lambda x: x.isna().sum()/len(x))\n",
    "    \n",
    "    # Keeping relevant columns\n",
    "    promise_columns = [column for column in filtered_promises.columns if f'promises{suffix}_' in column]\n",
    "    promise_columns_keep = ['transcriptid']\n",
    "    promise_columns_keep.extend(promise_columns)\n",
    "\n",
    "    return filtered_promises[promise_columns_keep]\n",
    "\n",
    "promises1 = promises[\n",
    "    ((promises['7-is-promise'] == 'yes') | (promises['7-is-promise'] == 'Yes')) &\n",
    "    ((promises['8-financial-guidance'] == 'no') | (promises['8-financial-guidance'] == 'No')) &\n",
    "    (promises['5-commitment-degree'] == 'strong-commitment') &\n",
    "    (promises['primary_keyword'] != 'Launch Announcements')\n",
    "]\n",
    "\n",
    "promises1 = process_promises(promises1, '_1')\n",
    "\n",
    "promises1.drop_duplicates(subset=['transcriptid'], keep='first', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "database = pd.merge(db, promises1, on='transcriptid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['promises_1_count_nolaunch'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.to_csv('../data/sxp1500_presentations_ceo_aggregated_regression_vars_cleaned_no_launch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcripts_wrds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
