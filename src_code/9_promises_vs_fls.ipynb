{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLS function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/majid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import ssl\n",
    "# Bypass SSL certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Ensure you have the required nltk resources\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_forward_looking(sentence, current_year):\n",
    "    # Define the keywords and conjugations for forward-looking criteria\n",
    "    keywords = [\n",
    "        \"will\", \"future\", \"next fiscal\", \"next month\", \"next period\", \"next quarter\", \"next year\",\n",
    "        \"incoming fiscal\", \"incoming month\", \"incoming period\", \"incoming quarter\", \"incoming year\",\n",
    "        \"coming fiscal\", \"coming month\", \"coming period\", \"coming quarter\", \"coming year\",\n",
    "        \"upcoming fiscal\", \"upcoming month\", \"upcoming period\", \"upcoming quarter\", \"upcoming year\",\n",
    "        \"subsequent fiscal\", \"subsequent month\", \"subsequent period\", \"subsequent quarter\", \"subsequent year\",\n",
    "        \"following fiscal\", \"following month\", \"following period\", \"following quarter\", \"following year\"\n",
    "    ]\n",
    "\n",
    "    excluded_keywords = [\"shall\", \"should\", \"can\", \"could\", \"may\", \"might\"]\n",
    "\n",
    "    verbs = [\n",
    "        \"aim\", \"anticipate\", \"assume\", \"commit\", \"estimate\", \"expect\",\n",
    "        \"forecast\", \"foresee\", \"hope\", \"intend\", \"plan\", \"project\",\n",
    "        \"seek\", \"target\"\n",
    "    ]\n",
    "\n",
    "    verb_conjugations = [\n",
    "        \"we \", \"and \", \"but \", \"do not \", \"company \", \"corporation \", \"firm \", \"management \",\n",
    "        \"and \", \"but \", \"does not \", \"is \", \"are \", \"not \", \"is \", \"are \", \"not \",\n",
    "        \"normally \", \"normally \", \"currently \", \"currently \", \"also \", \"also \"\n",
    "    ]\n",
    "\n",
    "    # Search 1: Keyword based search\n",
    "    for keyword in keywords:\n",
    "        if keyword in sentence and not any(excluded in sentence for excluded in excluded_keywords):\n",
    "            return True\n",
    "\n",
    "    # Search 2: Verb conjugation based search\n",
    "    for verb in verbs:\n",
    "        for conj in verb_conjugations:\n",
    "            if f\"{conj}{verb}\" in sentence:\n",
    "                return True\n",
    "\n",
    "    # Search 3: Year reference based search\n",
    "    year_matches = re.findall(r'\\b(20\\d{2})\\b', sentence)\n",
    "    for year in year_matches:\n",
    "        if int(year) > current_year:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def measure_forward_looking_statements(transcript, current_year):\n",
    "    sentences = sent_tokenize(transcript)\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "\n",
    "    forward_looking_sentences = 0\n",
    "    forward_looking_words = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if is_forward_looking(sentence, current_year):\n",
    "            forward_looking_sentences += 1\n",
    "            forward_looking_words += len(word_tokenize(sentence))\n",
    "\n",
    "    forward_looking_sentence_ratio = forward_looking_sentences / total_sentences if total_sentences > 0 else 0\n",
    "    forward_looking_word_ratio = forward_looking_words / total_words if total_words > 0 else 0\n",
    "\n",
    "    return forward_looking_sentence_ratio, forward_looking_word_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward-Looking Sentence Ratio: 0.0426\n",
      "Forward-Looking Word Ratio: 0.0323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "call_transcript = \"\"\"\n",
    "Thank you, Brett. It was a record third quarter powered by the continued strength of Microsoft Cloud, which surpassed $35 billion in revenue, up 23%. Microsoft Copilot and Copilot stack spanning everyday productivity, business process and developer services to models, data and infrastructure are orchestrating a new era of AI transformation driving better business outcomes across every role and industry. Now I'll highlight examples walking up the stack, starting with AI infrastructure.\n",
    "\n",
    "Azure again took share as customers use our platforms and tools to build their own AI solutions. We offer the most diverse selection of AI accelerators, including the latest from NVIDIA, AMD, as well as our own first-party silicon. Our AI innovation continues to build on our strategic partnership with OpenAI, more than 65% of the Fortune 500 now use Azure OpenAI service. We also continue to innovate and partner broadly to bring customers the best selection of frontier models in open-source models, LLMs, and SLMs with 53, which we announced earlier this week, we offer the most capable and cost-effective SLM available.\n",
    "\n",
    "It's already being trialed by companies like CallMiner, LTIMindtree, PwC, and TCS. Our models as a service offering makes it easy for developers to use LLM and SLM without having to manage any underlying infrastructure. Hundreds of paid customers from Accenture and EY to Schneider Electric are using it to take advantage of API access to third-party models, including as of this quarter, the latest from Cohere, Meta, and Mistral. And as part of our partnership announced last week, G42 will run its AI applications and services on our cloud.\n",
    "\n",
    "All up, the number of Azure AI customers continues to grow and average spend continues to increase. We also saw an acceleration of revenue from migrations to Azure. Azure Arc continues to help customers like DICK'S Sporting Goods and World Bank streamlined their cloud migrations. Arc now has 33,000 customers, up over 2x year over year, and we are the hyperscale platform of choice for SAP and Oracle workloads with Conduent and Medline moving their on-premise Oracle Estates to Azure and Kyndryl and L'Oreal migrating their SAP workloads to Azure.\n",
    "\n",
    "Overall, we are seeing an acceleration in the number of large Azure deals from leaders across industries, including billion-dollar-plus, multiyear commitments announced this month from Cloud Software Group and the Coca-Cola Company. The number of $100 million-plus Azure deals increased over 80% year over year, while the number of $10 million-plus deals more than doubled. Now on to data and analytics. Our Microsoft intelligent data platform provides customers with the broadest capability, spanning databases, analytics, business intelligence, governance, and AI.\n",
    "\n",
    "Over half of our Azure AI customers also use our data and analytics tools. Customers are building intelligent applications running on Azure, PostgreSQL, and Cosmos DB with deep integrations with Azure AI. TomTom is a great example. They've used Cosmos DB along with Azure Open AI service to build their own immersive in-car infotainment system.\n",
    "\n",
    "We are also encouraged by our momentum with our next-generation analytics platform, Microsoft Fabric. Fabric now has over 11,000 paid customers, including leaders in every industry from ABB, EDP, Energy Transfer to Equinor, Foot Locker, ITOCHU, and Lumen, and we are seeing increased usage intensity. Fabric is seamlessly integrated with Azure AI studio meaning customers can run models against enterprise data that's consolidated in Fabric's multi-cloud data lake, OneLake. And Power BI, which is also natively integrated with Fabric provides business users with AI-powered insights.\n",
    "\n",
    "We now have over 350,000 paid customers. Now on to developers. GitHub Copilot is bending the productivity curve for developers. We now have 1.8 million paid subscribers with growth accelerating to over 35% quarter over quarter and continues to see increased adoption from businesses in every industry, including Itau, Lufthansa Systems, Nokia, Pinterest, and Volvo cars.\n",
    "\n",
    "Copilot is driving growth across the broader GitHub platform, too. AT&T, Citigroup, and Honeywell all increased their overall getup usage after seeing productivity and code quality increases with Copilot. All up more than 90% of the Fortune 100 are now GitHub customers and revenue accelerated over 45% year over year. Anyone can be a developer with new AI-powered features across our low-code, no-code tools, which makes it easier to build an app, automate workflow or create a Copilot using natural language.\n",
    "\n",
    "Thirty thousand organizations across every industry have used Copilot studio to customize Copilot for Microsoft 365 or build their own, up 175% quarter over quarter. Cineplex, for example, built a Copilot for customer service agents, reducing query handling time from as much as 15 minutes to 30 seconds. All up over 330,000 organizations, including over half of Fortune 100 have used AI-powered capabilities in Power Platform, and Power Apps now has over 25 million monthly active users, up over 40% year over year. Now on to future of work.\n",
    "\n",
    "We are seeing AI democratize expertise across the workforce. What inventory turns are to efficiency of supply chains, knowledge turns, the creation and diffusion, and knowledge are to productivity of an organization and Copilot for Microsoft 365 is helping increase knowledge turns. Thus, having a cascading effect changing work, work artifacts, and workflows, and driving better decision-making, collaboration and efficiency. This quarter, we made Copilot available to organizations of all types and sizes from enterprises to small businesses, nearly 60% of the Fortune 500 now use Copilot and we have seen accelerated adoption across industries and geographies with companies like Amgen, BP, Cognizant, Koch Industries, Moody's, Novo Nordisk, NVIDIA, and Tech Mahindra purchasing over 10,000 seats.\n",
    "\n",
    "We're also seeing increased usage intensity from early adopters, including a nearly 50% increase in the number of Copilot-assisted interactions per user in Teams, bridging group activity with business process workflows and enterprise knowledge. And we're not stopping there. We're accelerating our innovation, adding over 150 Copilot capabilities since the start of the year. With Copilot in Dynamics 365, we are helping businesses transform every role in business function as we take share with our AI-powered apps across all categories.\n",
    "\n",
    "This quarter, we made our Copilot for service and Copilot for sales broadly available, helping customer service agents and sellers at companies like Land O'Lakes, Northern Trust, Rockwell Automation, and Toyota Group generate role-specific insights and recommendations from across Dynamics 365 and Microsoft 365, as well as third-party platforms like Salesforce, ServiceNow, and Zendesk. And with our Copilot for finance, we are drawing context from dynamics, as well as ERP systems like SAP to reduce labor-intensive processes like collections and contract and invoice capture for companies like dentsu and IDC. ISVs are also building their own Copilot integrations. For example, new integrations between Adobe Experience Cloud and Copilot will help marketers access campaign insights in the flow of their work.\n",
    "\n",
    "When it comes to devices, Copilot in Windows is now available on nearly 225 million Windows 10 and Windows 11 PCs, up two times quarter over quarter. With Copilot, we have an opportunity to create an entirely new category of devices, purpose built for this new generation of AI. All of our largest OEM partners have announced AI PCs in recent months. And this quarter, we introduced new surface devices, which include integrated NPUs to power on-device AI experiences like auto framing and live captions.\n",
    "\n",
    "And there's much more to come in just a few weeks, we'll hold a special event to talk about our AI vision across Windows and devices. When it comes to Teams, we once again saw year-over-year usage growth. We're rolling out a new version, which is up to two times faster while using 50% less memory for all customers. We surpassed 1 million Teams rooms for the first time as we continue to make hybrid meetings better with new AI-powered features like automatic camera switching and speaker recognition.\n",
    "\n",
    "And Teams Phone continues to be the market leader in cloud calling now with over 20 million PSTN users, up nearly 30% year over year. All of this innovation is driving growth across Microsoft 365 companies across the private and public sector, including Amadeus, BlackRock, Chevron, Ecolab, Kimberly Clark, all chose our premium E5 offerings this quarter for advanced security, compliance, voice, and analytics. Now on to industry and cross-industry clouds. We are also bringing AI-powered transformation to every industry.\n",
    "\n",
    "In healthcare, DAX Copilot is being used by more than 200 healthcare organizations, including Providence, Stanford Health Care, and WellSpan Health. And in manufacturing, this week, at HANNOVER MESSE, customers like BMW, Siemens, and Volvo Penta, shared how they're using our cloud and AI solutions to transform factory operations. Now on to security. Security underpins every layer of the tech stack and it's our No.\n",
    "\n",
    "1 priority. We launched our Secure Future Initiative last fall for this reason, bringing together every part of the company to advance cybersecurity protection and we are doubling down on this very important work, putting security about all else before all other features and investments. We are focused on making continuous progress across the six pillars of this initiative as we protect tenants and isolate production systems, protect identities and secrets, protect networks, protect engineering systems, monitor and detect threats, and accelerate responses and remediation. We remain committed to sharing our learnings, tools, and innovation with customers.\n",
    "\n",
    "A great example is Copilot for security, which we made generally available earlier this month, bringing together LLM with domain-specific skills informed by our threat intelligence and 78 trillion daily security signals to provide security teams with actionable insights. Now let me talk about our consumer businesses, starting with LinkedIn. We continue to combine our unique data with this new generation of AI to transform the way members learn, sell, and get hired. Features like LinkedIn AI-assisted messages are seeing a 40% higher acceptance rate and accepted over 10% faster by jobseekers saving hires, time and making it easier to connect them to candidates.\n",
    "\n",
    "Our AI-powered collaborative articles, which has reached over 12 million contributions are helping increase engagement on the platform, which reached a new record this quarter. New AI features are also helping accelerate LinkedIn premium growth with revenue up 29% year over year. We are also seeing strength across our other businesses with hiring, taking share for the seventh consecutive quarter. Now on to search advertising and news.\n",
    "\n",
    "We once again took share across Bing and Edge as we continue to apply this new generation of AI to transform how people search and browse. Bing reached over 140 million daily active users, and we are particularly encouraged by our momentum in mobile. Our free Copilot apps on iOS and Android saw a surge in downloads after our Super Bowl ad and are among the highest-rated in this category. We also rolled out Copilot to our ad platform this quarter, helping marketers use AI to generate recommendations for product images, headlines, and descriptions.\n",
    "\n",
    "Now on to gaming. We are committed to meeting players where they are by bringing great games to more people on more devices. We set third quarter records for game streaming hours, console usage, and monthly active devices. And last month, we added our first Activision Blizzard title Diablo 4 to our Game Pass service.\n",
    "\n",
    "Subscribers played over 10 million hours within the first 10 days, making it one of our biggest first-party Game Pass launches ever. We were also encouraged by ongoing success of Call of Duty: Modern Warfare 3, which is attracting new gamers and retaining franchise loyalists. Finally, we are expanding our games to new platforms, bringing four of our fan-favorite titles to Nintendo Switch and Sony PlayStation for the first time. In fact, earlier this month, we had seven games among the top 25 on the PlayStation store more than any other publisher.\n",
    "\n",
    "In closing, I'm energized about our opportunity ahead as we innovate to help people and businesses thrive in this new era. With that, let me turn it over to Amy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "current_year = 2024\n",
    "forward_looking_sentence_ratio, forward_looking_word_ratio = measure_forward_looking_statements(call_transcript, current_year)\n",
    "print(f\"Forward-Looking Sentence Ratio: {forward_looking_sentence_ratio:.4f}\")\n",
    "print(f\"Forward-Looking Word Ratio: {forward_looking_word_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_transcripts =pd.read_pickle(\"../data/sxp1500_presentations_ceo_aggregated.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcriptid</th>\n",
       "      <th>transcriptcomponentid_list</th>\n",
       "      <th>transcript_text</th>\n",
       "      <th>audiolengthsec</th>\n",
       "      <th>companyid</th>\n",
       "      <th>companyname</th>\n",
       "      <th>companyofperson</th>\n",
       "      <th>componentorder</th>\n",
       "      <th>componenttextpreview</th>\n",
       "      <th>delayreasontypeid</th>\n",
       "      <th>...</th>\n",
       "      <th>transcriptcreationtime_utc</th>\n",
       "      <th>transcriptpersonid</th>\n",
       "      <th>transcriptpersonname</th>\n",
       "      <th>transcriptpresentationtypeid</th>\n",
       "      <th>transcriptpresentationtypename</th>\n",
       "      <th>word_count</th>\n",
       "      <th>year</th>\n",
       "      <th>transcript_text_len</th>\n",
       "      <th>transcriptcomponentid_ceospeech</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45107.0</td>\n",
       "      <td>[3450953.0]</td>\n",
       "      <td>Thanks, Doug, and good afternoon everyone. Tha...</td>\n",
       "      <td>None</td>\n",
       "      <td>4803544.0</td>\n",
       "      <td>AngioDynamics, Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>Thanks, Doug, and good afternoon everyone. Tha...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>04:28:29.000000</td>\n",
       "      <td>130722.0</td>\n",
       "      <td>Johannes C. Keltjens</td>\n",
       "      <td>5</td>\n",
       "      <td>Final</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>8651</td>\n",
       "      <td>3450953.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45212.0</td>\n",
       "      <td>[3457811.0, 3457813.0]</td>\n",
       "      <td>Thank you, John, and good morning to everyone,...</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>189424.0</td>\n",
       "      <td>Team, Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Thank you, John, and good morning to everyone,...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>23:55:33.000000</td>\n",
       "      <td>116628.0</td>\n",
       "      <td>Philip Hawk</td>\n",
       "      <td>5</td>\n",
       "      <td>Final</td>\n",
       "      <td>166.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>12012</td>\n",
       "      <td>3457811.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45220.0</td>\n",
       "      <td>[3458448.0, 3458450.0]</td>\n",
       "      <td>Thank you, Jasmine, and good morning. Welcome ...</td>\n",
       "      <td>5940.0</td>\n",
       "      <td>299095.0</td>\n",
       "      <td>RPM International Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Thank you, Jasmine, and good morning. Welcome ...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>00:36:48.000000</td>\n",
       "      <td>116583.0</td>\n",
       "      <td>Frank Sullivan</td>\n",
       "      <td>5</td>\n",
       "      <td>Final</td>\n",
       "      <td>435.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>4481</td>\n",
       "      <td>3458448.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45248.0</td>\n",
       "      <td>[3460066.0, 3460069.0]</td>\n",
       "      <td>Thank you, Cathy, and good afternoon, everyone...</td>\n",
       "      <td>3480.0</td>\n",
       "      <td>314750.0</td>\n",
       "      <td>Worthington Industries, Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>Thank you, Cathy, and good afternoon, everyone...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>06:13:10.000000</td>\n",
       "      <td>102758.0</td>\n",
       "      <td>John McConnell</td>\n",
       "      <td>5</td>\n",
       "      <td>Final</td>\n",
       "      <td>217.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3332</td>\n",
       "      <td>3460066.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45259.0</td>\n",
       "      <td>[3460878.0]</td>\n",
       "      <td>Thank you, Len. Good afternoon, everyone and t...</td>\n",
       "      <td>2820.0</td>\n",
       "      <td>321778.0</td>\n",
       "      <td>Bed Bath &amp; Beyond Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>Thank you, Len. Good afternoon, everyone and t...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>08:37:54.000000</td>\n",
       "      <td>116225.0</td>\n",
       "      <td>Steven Temares</td>\n",
       "      <td>5</td>\n",
       "      <td>Final</td>\n",
       "      <td>819.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>5320</td>\n",
       "      <td>3460878.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   transcriptid transcriptcomponentid_list   \n",
       "0       45107.0                [3450953.0]  \\\n",
       "1       45212.0     [3457811.0, 3457813.0]   \n",
       "2       45220.0     [3458448.0, 3458450.0]   \n",
       "3       45248.0     [3460066.0, 3460069.0]   \n",
       "4       45259.0                [3460878.0]   \n",
       "\n",
       "                                     transcript_text audiolengthsec   \n",
       "0  Thanks, Doug, and good afternoon everyone. Tha...           None  \\\n",
       "1  Thank you, John, and good morning to everyone,...         3600.0   \n",
       "2  Thank you, Jasmine, and good morning. Welcome ...         5940.0   \n",
       "3  Thank you, Cathy, and good afternoon, everyone...         3480.0   \n",
       "4  Thank you, Len. Good afternoon, everyone and t...         2820.0   \n",
       "\n",
       "   companyid                   companyname companyofperson componentorder   \n",
       "0  4803544.0           AngioDynamics, Inc.            None              3  \\\n",
       "1   189424.0                    Team, Inc.            None              2   \n",
       "2   299095.0        RPM International Inc.            None              2   \n",
       "3   314750.0  Worthington Industries, Inc.            None              3   \n",
       "4   321778.0        Bed Bath & Beyond Inc.            None              4   \n",
       "\n",
       "                                componenttextpreview delayreasontypeid  ...   \n",
       "0  Thanks, Doug, and good afternoon everyone. Tha...              None  ...  \\\n",
       "1  Thank you, John, and good morning to everyone,...              None  ...   \n",
       "2  Thank you, Jasmine, and good morning. Welcome ...              None  ...   \n",
       "3  Thank you, Cathy, and good afternoon, everyone...              None  ...   \n",
       "4  Thank you, Len. Good afternoon, everyone and t...              None  ...   \n",
       "\n",
       "  transcriptcreationtime_utc transcriptpersonid  transcriptpersonname   \n",
       "0            04:28:29.000000           130722.0  Johannes C. Keltjens  \\\n",
       "1            23:55:33.000000           116628.0           Philip Hawk   \n",
       "2            00:36:48.000000           116583.0        Frank Sullivan   \n",
       "3            06:13:10.000000           102758.0        John McConnell   \n",
       "4            08:37:54.000000           116225.0        Steven Temares   \n",
       "\n",
       "   transcriptpresentationtypeid transcriptpresentationtypename  word_count   \n",
       "0                             5                          Final      1425.0  \\\n",
       "1                             5                          Final       166.0   \n",
       "2                             5                          Final       435.0   \n",
       "3                             5                          Final       217.0   \n",
       "4                             5                          Final       819.0   \n",
       "\n",
       "   year  transcript_text_len transcriptcomponentid_ceospeech  processed  \n",
       "0  2010                 8651                       3450953.0          0  \n",
       "1  2010                12012                       3457811.0          0  \n",
       "2  2010                 4481                       3458448.0          0  \n",
       "3  2010                 3332                       3460066.0          0  \n",
       "4  2010                 5320                       3460878.0          0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_transcripts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function on the first transcript\n",
    "forward_looking_sentence_ratio, forward_looking_word_ratio = measure_forward_looking_statements(aggregated_transcripts['transcript_text'][0], aggregated_transcripts['year'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are there duplicates for transcriptid?\n",
    "aggregated_transcripts['transcriptid'].nunique() == aggregated_transcripts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to all transcripts, save in two new columns the forward looking sentence ratio and the forward looking word ratio\n",
    "aggregated_transcripts[['forward_looking_sentence_ratio', 'forward_looking_word_ratio']] = aggregated_transcripts.apply(lambda x: pd.Series(measure_forward_looking_statements(x['transcript_text'], x['year'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       forward_looking_sentence_ratio  forward_looking_word_ratio\n",
      "count                    69248.000000                69248.000000\n",
      "mean                         0.162004                    0.188204\n",
      "std                          0.081321                    0.095204\n",
      "min                          0.000000                    0.000000\n",
      "25%                          0.103448                    0.120318\n",
      "50%                          0.153846                    0.178492\n",
      "75%                          0.210526                    0.245239\n",
      "max                          0.764706                    0.872093\n"
     ]
    }
   ],
   "source": [
    "# print summary statistics of the forward looking sentence ratio and the forward looking word ratio\n",
    "print(aggregated_transcripts[['forward_looking_sentence_ratio', 'forward_looking_word_ratio']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe with the new columns; only transcriptid and the two new columns\n",
    "aggregated_transcripts[['transcriptid', 'forward_looking_sentence_ratio', 'forward_looking_word_ratio']].to_pickle(\"../data/sxp1500_presentations_ceo_aggregated_forward_looking.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLS in promises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/1pq07r555gxgt22gqksl2tlw0000gn/T/ipykernel_41643/4137992819.py:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  promises = pd.read_csv('../data/LIWC-22 Results - sxp1500_presentations_ceo_aggr___ - LIWC Analysis_v11_horizon_v2_specificity.csv')\n"
     ]
    }
   ],
   "source": [
    "# open file\n",
    "promises = pd.read_csv('../data/sxp1500_presentations_ceo_promises_horizon_specificity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transcriptid', 'companyname', 'gvkey', 'transcript_date',\n",
       "       'speaker_name', 'presentation_len', 'year', 'full_transcript_len',\n",
       "       'promise_verbatim', 'promise_explain', 'promise_id',\n",
       "       'promise_horizon_months', 'negative_count', 'positive_count',\n",
       "       'uncertainty_count', 'total_word_count', 'negative_ratio',\n",
       "       'positive_ratio', 'uncertainty_ratio', 'sentiment_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promises.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the measure_forward_looking_statements(x['promise_verbatim'], x['year']) to each row\n",
    "promises[['forward_looking_sentence_ratio', 'forward_looking_word_ratio']] = promises.apply(lambda x: pd.Series(measure_forward_looking_statements(x['promise_verbatim'], x['year'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    174617.000000\n",
       "mean          0.599708\n",
       "std           0.456508\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           1.000000\n",
       "75%           1.000000\n",
       "max           1.000000\n",
       "Name: forward_looking_sentence_ratio, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promises['forward_looking_sentence_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group promises together per transcriptid. concatenate the promise_verbatim\n",
    "promises_grouped = promises.groupby('transcriptid').agg({'promise_verbatim': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the promises_grouped with the aggregated_transcripts on transcriptid\n",
    "aggregated_transcripts = aggregated_transcripts.merge(promises_grouped, on='transcriptid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def is_forward_looking(sentence, current_year):\n",
    "    keywords = [\n",
    "        \"will\", \"future\", \"next fiscal\", \"next month\", \"next period\", \"next quarter\", \"next year\",\n",
    "        \"incoming fiscal\", \"incoming month\", \"incoming period\", \"incoming quarter\", \"incoming year\",\n",
    "        \"coming fiscal\", \"coming month\", \"coming period\", \"coming quarter\", \"coming year\",\n",
    "        \"upcoming fiscal\", \"upcoming month\", \"upcoming period\", \"upcoming quarter\", \"upcoming year\",\n",
    "        \"subsequent fiscal\", \"subsequent month\", \"subsequent period\", \"subsequent quarter\", \"subsequent year\",\n",
    "        \"following fiscal\", \"following month\", \"following period\", \"following quarter\", \"following year\"\n",
    "    ]\n",
    "\n",
    "    excluded_keywords = [\"shall\", \"should\", \"can\", \"could\", \"may\", \"might\"]\n",
    "\n",
    "    verbs = [\n",
    "        \"aim\", \"anticipate\", \"assume\", \"commit\", \"estimate\", \"expect\",\n",
    "        \"forecast\", \"foresee\", \"hope\", \"intend\", \"plan\", \"project\",\n",
    "        \"seek\", \"target\"\n",
    "    ]\n",
    "\n",
    "    verb_conjugations = [\n",
    "        \"we \", \"and \", \"but \", \"do not \", \"company \", \"corporation \", \"firm \", \"management \",\n",
    "        \"and \", \"but \", \"does not \", \"is \", \"are \", \"not \", \"is \", \"are \", \"not \",\n",
    "        \"normally \", \"normally \", \"currently \", \"currently \", \"also \", \"also \"\n",
    "    ]\n",
    "\n",
    "    for keyword in keywords:\n",
    "        if keyword in sentence and not any(excluded in sentence for excluded in excluded_keywords):\n",
    "            return True\n",
    "\n",
    "    for verb in verbs:\n",
    "        for conj in verb_conjugations:\n",
    "            if f\"{conj}{verb}\" in sentence:\n",
    "                return True\n",
    "\n",
    "    year_matches = re.findall(r'\\b(20\\d{2})\\b', sentence)\n",
    "    for year in year_matches:\n",
    "        if int(year) > current_year:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def measure_forward_looking_statements(transcript, current_year, given_text):\n",
    "    transcript_sentences = sent_tokenize(transcript)\n",
    "    given_text_sentences = sent_tokenize(given_text)\n",
    "\n",
    "    total_sentences = len(transcript_sentences)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in transcript_sentences)\n",
    "\n",
    "    forward_looking_sentences = 0\n",
    "    forward_looking_words = 0\n",
    "    fls_in_given_text = 0\n",
    "    fls_not_in_given_text = 0\n",
    "\n",
    "    def normalize_sentence(sentence):\n",
    "        return re.sub(r'\\s+', ' ', sentence.strip())\n",
    "\n",
    "    given_text_sentences_normalized = set(normalize_sentence(s) for s in given_text_sentences)\n",
    "\n",
    "    for sentence in transcript_sentences:\n",
    "        normalized_sentence = normalize_sentence(sentence)\n",
    "        is_fls = is_forward_looking(sentence, current_year)\n",
    "        in_given_text = normalized_sentence in given_text_sentences_normalized\n",
    "\n",
    "        if is_fls:\n",
    "            forward_looking_sentences += 1\n",
    "            forward_looking_words += len(word_tokenize(sentence))\n",
    "            if in_given_text:\n",
    "                fls_in_given_text += 1\n",
    "            else:\n",
    "                fls_not_in_given_text += 1\n",
    "\n",
    "    forward_looking_sentence_ratio = forward_looking_sentences / total_sentences if total_sentences > 0 else 0\n",
    "    forward_looking_word_ratio = forward_looking_words / total_words if total_words > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"forward_looking_sentence_ratio\": forward_looking_sentence_ratio,\n",
    "        \"forward_looking_word_ratio\": forward_looking_word_ratio,\n",
    "        \"fls_in_given_text\": fls_in_given_text,\n",
    "        \"fls_not_in_given_text\": fls_not_in_given_text,\n",
    "        \"total_sentences\": total_sentences,\n",
    "        \"total_words\": total_words\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forward_looking_sentence_ratio': 0.625, 'forward_looking_word_ratio': 0.6774193548387096, 'fls_in_given_text': 3, 'fls_not_in_given_text': 2, 'total_sentences': 8, 'total_words': 93}\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "transcript = \"\"\"\n",
    "Our company will achieve significant growth next year. We plan to expand our operations in the next quarter. However, we should be cautious about market fluctuations. Our forecast predicts a 20% increase in revenue by the end of 2025. Currently, we are focusing on improving our product line. The incoming fiscal year will bring new opportunities for innovation and expansion. Our management anticipates a successful launch of our new product line. Although the current period is challenging, the future looks promising.\n",
    "\"\"\"\n",
    "\n",
    "given_text = \"\"\"\n",
    "Our company will achieve significant growth next year. We plan to expand our operations in the next quarter. Our forecast predicts a 20% increase in revenue by the end of 2025. The incoming fiscal year will bring new opportunities for innovation and expansion.\n",
    "\"\"\"\n",
    "\n",
    "current_year = 2024\n",
    "\n",
    "# Function call\n",
    "result = measure_forward_looking_statements(transcript, current_year, given_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure tqdm works with pandas apply\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with empty strings and convert necessary columns to strings\n",
    "aggregated_transcripts['transcript_text'] = aggregated_transcripts['transcript_text'].fillna('').astype(str)\n",
    "aggregated_transcripts['promise_verbatim'] = aggregated_transcripts['promise_verbatim'].fillna('').astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69248/69248 [12:33<00:00, 91.89it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now apply the function to each row with a progress bar\n",
    "aggregated_transcripts[['forward_looking_sentence_ratio', 'forward_looking_word_ratio', 'fls_in_given_text', 'fls_not_in_given_text', 'total_sentences', 'total_words']] = aggregated_transcripts.progress_apply(\n",
    "    lambda x: pd.Series(measure_forward_looking_statements(x['transcript_text'], x['year'], x['promise_verbatim'])), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    69248.000000\n",
       "mean         0.174381\n",
       "std          0.086062\n",
       "min          0.000000\n",
       "25%          0.112500\n",
       "50%          0.166667\n",
       "75%          0.226087\n",
       "max          0.800000\n",
       "Name: forward_looking_sentence_ratio, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_transcripts['forward_looking_sentence_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_transcripts['fls_promise_ratio'] = aggregated_transcripts['fls_in_given_text'] / aggregated_transcripts['total_sentences']\n",
    "aggregated_transcripts['fls_nonpromise_ratio'] = aggregated_transcripts['fls_not_in_given_text'] / aggregated_transcripts['total_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    69248.000000\n",
       "mean         0.023843\n",
       "std          0.028517\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.017241\n",
       "75%          0.035714\n",
       "max          0.368421\n",
       "Name: fls_promise_ratio, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_transcripts['fls_promise_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    69248.000000\n",
       "mean         0.150538\n",
       "std          0.079994\n",
       "min          0.000000\n",
       "25%          0.093023\n",
       "50%          0.141732\n",
       "75%          0.197802\n",
       "max          0.800000\n",
       "Name: fls_nonpromise_ratio, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_transcripts['fls_nonpromise_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8632720532935787"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean ratio of fls_nonpromise to fls_promise+fls_nonpromise\n",
    "aggregated_transcripts['fls_nonpromise_ratio'].mean() / (aggregated_transcripts['fls_promise_ratio'].mean() + aggregated_transcripts['fls_nonpromise_ratio'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    69248.000000\n",
       "mean        62.682864\n",
       "std         35.059009\n",
       "min          1.000000\n",
       "25%         39.000000\n",
       "50%         57.000000\n",
       "75%         79.000000\n",
       "max        772.000000\n",
       "Name: total_sentences, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_transcripts['total_sentences'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcripts_wrds_jupyter2",
   "language": "python",
   "name": "transcripts_wrds_jupyter2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
